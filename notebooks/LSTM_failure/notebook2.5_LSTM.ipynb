{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align='center'>\n",
    "<font size=\"+2\">\n",
    "\n",
    "TM&NLP - Project\n",
    "\n",
    "<b>Classification Between Multiple Choices </b>\n",
    "\n",
    "Ghiotto Alessandro 513944\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 - Contextualized Embeddings:\n",
    "\n",
    "# **TO BE UPDATED**\n",
    "\n",
    "1. Representation by means of static word embeddings:\n",
    "    - Word2Vec\n",
    "    - GloVE\n",
    "    - FastText\n",
    "    - Doc2Vec\n",
    "2. Other ways of combining word embeddings:\n",
    "    - Remove duplicated words\n",
    "    - Word embeddings weighted by their idf score\n",
    "3. Other ways of choosing the answer:\n",
    "    - Siamese Neural Network\n",
    "    - Feed Forward Neural Network\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968aa5d687644365a93446940940ef7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7323 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6620c232afb4f6d937136db645ec08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '3E7TUJ2EGCLQNOV1WEAJ2NN9ROPD9K',\n",
       " 'question': 'What type of water formation is formed by clouds?',\n",
       " 'choices': ['pearls',\n",
       "  'streams',\n",
       "  'shells',\n",
       "  'diamonds',\n",
       "  'rain',\n",
       "  'beads',\n",
       "  'cooled',\n",
       "  'liquid'],\n",
       " 'answerKey': 'F',\n",
       " 'fact1': 'beads of water are formed by water vapor condensing',\n",
       " 'fact2': 'Clouds are made of water vapor.',\n",
       " 'combinedfact': 'Beads of water can be formed by clouds.',\n",
       " 'formatted_question': 'What type of water formation is formed by clouds? (A) pearls (B) streams (C) shells (D) diamonds (E) rain (F) beads (G) cooled (H) liquid'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# set the seed\n",
    "seed = 8\n",
    "set_seed(seed)\n",
    "\n",
    "# set gpu as default device and float32 as default dtype\n",
    "mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(mydevice) # default tensor device\n",
    "torch.set_default_dtype(torch.float32) # default tensor dtype\n",
    "\n",
    "dataset = load_dataset(\"allenai/qasc\")\n",
    "\n",
    "n_train_sample = 7323\n",
    "dataset_train = dataset['train'].select(range(n_train_sample))\n",
    "dataset_val = dataset['train'].select(range(n_train_sample, len(dataset['train'])))\n",
    "dataset_test = dataset['validation']\n",
    "\n",
    "def format_choices(example):\n",
    "    # transform the choices from a dictionary to a list of strings\n",
    "    # I will eliminate the labels, if we know that the order is always the same\n",
    "    # Does all the samples have the same order of choices?\n",
    "    if example['choices']['label'] == ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']:\n",
    "        # get the text of the choices\n",
    "        example['choices'] = example['choices']['text']\n",
    "    else:\n",
    "        print(\"The order of the choices is not the same for all the examples\")\n",
    "    return example\n",
    "\n",
    "dataset_train = dataset_train.map(format_choices)\n",
    "dataset_val = dataset_val.map(format_choices)\n",
    "dataset_test = dataset_test.map(format_choices)\n",
    "\n",
    "# Display the dataset\n",
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **BidirectionalLSTM**\n",
    "\n",
    "As contextualized word embeddings I will take the output of the BidirectionalLSTM Neural Network. Then I will feed it to a classification head (MLP). The task is Binary classification (1 for the correct choice, 0 otherwise), `loss = 'binary_cross_entropy'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to build the tokenizer, I will use **WordPiece** tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we build the tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "What is the capital of France?\n",
      "Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "[104, 118, 106, 1804, 124, 108, 114, 4528, 159, 25]\n",
      "['what', 'is', 'the', 'cap', '##it', '##al', 'of', 'fran', '##ce', '?']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "vocab_size = 15000\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# decoding technique: how to put back together our subwords\n",
    "tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "# we can set various special tokens (extra) useful for handling specific cases if needed.\n",
    "trainer = WordPieceTrainer(vocab_size=vocab_size,\n",
    "                     special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase\n",
    "\n",
    "# PREPROCESSING STEPS\n",
    "# before tokenization we lowercase all words\n",
    "tokenizer.normalizer = Lowercase()\n",
    "# This pre-tokenizer simply splits using the following regex: \\w+|[^\\w\\s]+ (spaces and punctuations)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# TRAIN\n",
    "# generate a text file containing all the texts\n",
    "with open(\"tokenizers/qasc_textForTokenizer.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # remove duplicates (a lot of facts are repeated)\n",
    "    train_texts = list(set(dataset_train[\"question\"] + dataset_train[\"fact1\"] + dataset_train[\"fact2\"]))\n",
    "    for i in range(len(train_texts)):\n",
    "        f.write(train_texts[i] + \"\\n\")\n",
    "\n",
    "train_file = \"tokenizers/qasc_textForTokenizer.txt\"\n",
    "files = [train_file]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# save as a json file (so we can use it for later for example)\n",
    "saved_tokenizer = \"tokenizers/WordPiece_tokenizer.json\"\n",
    "tokenizer.save(saved_tokenizer)\n",
    "\n",
    "### EXAMPLE\n",
    "# load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(saved_tokenizer)\n",
    "# decoding technique: how to put back together our subwords\n",
    "tokenizer.decoder = decoders.WordPiece()\n",
    "# encode a sentence\n",
    "sentence = \"What is the capital of France?\"\n",
    "encoded = tokenizer.encode(sentence)\n",
    "print(sentence)\n",
    "print(encoded)\n",
    "print(encoded.ids)\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD ID: 0\n",
      "VOCABULARY SIZE: 11829\n",
      "CHECK PADDING IDX: 0\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 300\n",
    "\n",
    "# MAKE IT FAST + PADDING + TRUNCATION\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(saved_tokenizer)\n",
    "\n",
    "# Enabling padding with the right chosen index\n",
    "pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "print(\"PAD ID: \"+str(pad_id))\n",
    "tokenizer.enable_padding(pad_id=pad_id, pad_token=\"[PAD]\")\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "# we can use a fast version of our tokenizer\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "# we can set the cut-off threshold for how long (i.e. how many words in a sentence) we want to handle our sentences\n",
    "# all the words after MAX_LEGTH are just remove (truncate excessive contexts) (also for memory reasons)\n",
    "fast_tokenizer.model_max_length = MAX_LENGTH\n",
    "# truncate on the left, so I keep the choice which is at the end\n",
    "fast_tokenizer.truncation_side = 'left'\n",
    "\n",
    "vocab_size = len(fast_tokenizer)\n",
    "print(\"VOCABULARY SIZE: \"+str(vocab_size))\n",
    "padding_idx = fast_tokenizer.pad_token_id\n",
    "print(\"CHECK PADDING IDX: \"+str(padding_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this sentences of the dataset are quite specific, we have reached a vocabulary size equal to the number of unique words in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c6cd8ef7594866923aeb0b32e48883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7323 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  58584\n",
      "number of tokens in the first sample:  28\n",
      "first sample:  tensor([ 104,  530,  114,  183, 2361,  118,  408,  160, 1324,   25, 3601,  114,\n",
      "         183,  121,  408,  160,  183,  733, 1924, 1324,  121,  504,  114,  183,\n",
      "         733,   11,  513, 8689], device='cuda:0')\n",
      "decoded:  what type of water formation is formed by clouds? beads of water are formed by water vapor condensing clouds are made of water vapor. pearls\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZE THE FULL TRAINING DATA\n",
    "def preprocess_function(examples):\n",
    "    # question + fact1 + fact2 + choice\n",
    "    # we do it for all choices (for each example)\n",
    "    sentences = [\n",
    "        f\"{examples['question']} {examples['fact1']} {examples['fact2']} {examples['choices'][choice_idx]}\" \n",
    "        for choice_idx in range(8)\n",
    "    ]\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_examples = fast_tokenizer(sentences, truncation=True)\n",
    "    # I know that for each sample I have 8 choices\n",
    "    # 0->A, 1->B, 2->C, 3->D, 4->E, 5->F, 6->G, 7->H\n",
    "    # 8->A, ...\n",
    "    return tokenized_examples\n",
    "\n",
    "dataset_train_inputids = dataset_train.map(preprocess_function).with_format('torch')['input_ids']\n",
    "dataset_train_inputids = [inputid for row in dataset_train_inputids for inputid in row]\n",
    "print('number of samples: ', len(dataset_train_inputids))\n",
    "example = dataset_train_inputids[0]\n",
    "print('number of tokens in the first sample: ', len(example))\n",
    "print('first sample: ', example)\n",
    "print('decoded: ', fast_tokenizer.decode(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d23cd51e31e4af886532fbe38173160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7323 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train_set len: 58584\n",
      "[0, 0, 0, 0, 0, 1, 0, 0] corresponding to the first example F\n",
      "[0, 0, 0, 0, 1, 0, 0, 0] corresponding to the second example E\n",
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 52725\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# CREATE THE LABELS\n",
    "# for each answerKey I create 8 labels (one for each choice)\n",
    "# 0->A, 1->B, 2->C, 3->D, 4->E, 5->F, 6->G, 7->H\n",
    "# 1 if the choice is correct, 0 otherwise\n",
    "def create_labels(example):\n",
    "    labels = []\n",
    "    for i in range(8):\n",
    "        if example['answerKey'] == chr(65+i):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    example['label'] = labels\n",
    "    return example\n",
    "\n",
    "# apply the function to the dataset and then flatten the list\n",
    "# so that we have a list of length 8*len(dataset)\n",
    "# I didn't used batches for semplicity (don't have many samples, and it's a simple operation)\n",
    "labels = dataset_train.map(create_labels, remove_columns=dataset_train.column_names)\n",
    "labels = [label for row in labels[\"label\"] for label in row]\n",
    "print('original train_set len:', len(labels))\n",
    "print(labels[:8], 'corresponding to the first example', dataset_train['answerKey'][0])\n",
    "print(labels[8:16], 'corresponding to the second example', dataset_train['answerKey'][1])\n",
    "\n",
    "# CREATE THE DATASET\n",
    "# I create a dictionary with the input_ids and the labels\n",
    "# I will use this dictionary to create the Dataset object\n",
    "data = {'input_ids': dataset_train_inputids, 'labels': labels}\n",
    "data_train = Dataset.from_dict(data).with_format('torch')\n",
    "\n",
    "# take a portion of the data for validation\n",
    "data_val = data_train.train_test_split(test_size=0.1, seed=seed, shuffle=True)\n",
    "data_train = data_val['train']\n",
    "data_val = data_val['test']\n",
    "\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tokenized the dataset and created the labels we can build the model, contituted by 2 BiLSTM layers + a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTM_Classifier(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx, out_features=128, lstm_layers=1):\n",
    "        # num_embeddings : size of the dictionary of embeddings\n",
    "\t\t# embedding_dim  : the size of each embedding vector\n",
    "\t\t# padding_idx : If specified, the entries at padding_idx do not contribute to the gradient\n",
    "\t\t# out_features : size of the output features\n",
    "\t\t# lstm_layers : number of recurrent layers\n",
    "\t\t\n",
    "\t\t# the embedding vector at padding_idx is not updated during training, it remains as a fixed “pad”\n",
    "        super(BiLSTM_Classifier, self).__init__()\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=embedding_dim, num_layers=lstm_layers,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_dim * 2, out_features=out_features)\n",
    "        self.fc2 = nn.Linear(out_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize the hidden state and the cell state\n",
    "        # batch_size = x.size(0)\n",
    "        # h = torch.zeros((self.lstm_layers * 2, batch_size, self.embedding_dim)).to(x.device)\n",
    "        # c = torch.zeros((self.lstm_layers * 2, batch_size, self.embedding_dim)).to(x.device)\n",
    "        # torch.nn.init.xavier_normal_(h)\n",
    "\t\t# torch.nn.init.xavier_normal_(c)\n",
    "\n",
    "        embs = self.embedding_layer(x)\n",
    "        out_lstm, (h_n, c_n) = self.lstm(embs)#, (h, c))\n",
    "\n",
    "        # out_lstm : [batch_size, seq_len, 2*hidden_size]\n",
    "\t\t# the last element of out_lstm contains the final forward hidden state and the initial reverse hidden state\n",
    "        # Get the last hidden state\n",
    "        out = out_lstm[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "BiLSTM_Classifier                        --\n",
      "├─Embedding: 1-1                         3,028,224\n",
      "├─LSTM: 1-2                              2,629,632\n",
      "├─Dropout: 1-3                           --\n",
      "├─Linear: 1-4                            65,664\n",
      "├─Linear: 1-5                            129\n",
      "=================================================================\n",
      "Total params: 5,723,649\n",
      "Trainable params: 5,723,649\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "EPOCH 0\n",
      "Epoch 0  , iter 100  - train_loss 0.3452, val_loss 0.7547\n",
      "Epoch 0  , iter 200  - train_loss 0.3620, val_loss 0.7551\n",
      "Epoch 0  , iter 300  - train_loss 0.3828, val_loss 0.7549\n",
      "Epoch 0  , iter 400  - train_loss 0.3628, val_loss 0.7557\n",
      "EPOCH 1\n",
      "Epoch 1  , iter 500  - train_loss 0.3944, val_loss 0.7548\n",
      "Epoch 1  , iter 600  - train_loss 0.3888, val_loss 0.7546\n",
      "Epoch 1  , iter 700  - train_loss 0.3884, val_loss 0.7562\n",
      "Epoch 1  , iter 800  - train_loss 0.4213, val_loss 0.7534\n",
      "EPOCH 2\n",
      "Epoch 2  , iter 900  - train_loss 0.3193, val_loss 0.7549\n",
      "Epoch 2  , iter 1000 - train_loss 0.3345, val_loss 0.7616\n",
      "Epoch 2  , iter 1100 - train_loss 0.3432, val_loss 0.7519\n",
      "Epoch 2  , iter 1200 - train_loss 0.4201, val_loss 0.7574\n",
      "EPOCH 3\n",
      "Epoch 3  , iter 1300 - train_loss 0.3257, val_loss 0.7529\n",
      "Epoch 3  , iter 1400 - train_loss 0.3055, val_loss 0.7545\n",
      "Epoch 3  , iter 1500 - train_loss 0.3454, val_loss 0.7509\n",
      "Epoch 3  , iter 1600 - train_loss 0.3566, val_loss 0.7512\n",
      "EPOCH 4\n",
      "Epoch 4  , iter 1700 - train_loss 0.4336, val_loss 0.7512\n",
      "Epoch 4  , iter 1800 - train_loss 0.3129, val_loss 0.7495\n",
      "Epoch 4  , iter 1900 - train_loss 0.2796, val_loss 0.7485\n",
      "Epoch 4  , iter 2000 - train_loss 0.2637, val_loss 0.7502\n",
      "EPOCH 5\n",
      "Epoch 5  , iter 2100 - train_loss 0.3999, val_loss 0.7547\n",
      "Epoch 5  , iter 2200 - train_loss 0.2950, val_loss 0.7517\n",
      "Epoch 5  , iter 2300 - train_loss 0.3448, val_loss 0.7468\n",
      "Epoch 5  , iter 2400 - train_loss 0.4874, val_loss 0.7566\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "embedding_dim = 256\n",
    "out_features = 128\n",
    "n_lstm_layers = 2\n",
    "learning_rate = 5*1e-5\n",
    "batch_size = 128\n",
    "nb_epoch = 6\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torchinfo import summary\n",
    "\n",
    "# DataCollatorWithPadding will add the Padding automatically (saved in our tokenizer)\n",
    "datacollator = DataCollatorWithPadding(tokenizer=fast_tokenizer)\n",
    "\n",
    "# DataLoader : take care of loading batch of data from our training data\n",
    "generator = torch.Generator(device=mydevice)\n",
    "dataloader_train = DataLoader(data_train, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=datacollator, generator=generator)\n",
    "dataloader_val = DataLoader(data_val, batch_size=batch_size, drop_last=False, shuffle=False, collate_fn=datacollator, generator=generator)\n",
    "\n",
    "# instantiate the model\n",
    "# vocab_size and padding_idx are taken from the tokenizer (saved before)\n",
    "BiLSTM_model = BiLSTM_Classifier(vocab_size, embedding_dim, padding_idx, out_features=out_features, lstm_layers=n_lstm_layers)\n",
    "BiLSTM_model.to(mydevice)\n",
    "# PRINT THE MODEL ARCHITECTURE\n",
    "print(summary(BiLSTM_model))\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = torch.optim.Adam(BiLSTM_model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "BiLSTM_model.train()\n",
    "\n",
    "counter = []\n",
    "loss_history_train = [] \n",
    "loss_history_val = []\n",
    "i = 0\n",
    "for epoch in range(nb_epoch):\n",
    "    print(\"EPOCH \"+str(epoch))\n",
    "    for item in dataloader_train:#, disable=True):\n",
    "        optimizer.zero_grad()\n",
    "        item = item.to(mydevice) # for GPU\n",
    "        # target = just the input, but start from the second word\n",
    "        inp_ids, labels = item['input_ids'], item['labels']\n",
    "\n",
    "        # output of the sigmoid is the probability of the correct class\n",
    "        output = BiLSTM_model(inp_ids)\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(output, labels.float().unsqueeze(1))\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        i += 1\n",
    "        # Every 100 batches print out the loss + save the loss\n",
    "        if i % 100 == 0 :\n",
    "            # compute the loss on the validation set\n",
    "            BiLSTM_model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for item in dataloader_val:\n",
    "                    item = item.to(mydevice)\n",
    "                    inp_ids, labels = item['input_ids'], item['labels']\n",
    "                    output = BiLSTM_model(inp_ids)\n",
    "                    val_loss += loss_fn(output, labels.float().unsqueeze(1))\n",
    "                    val_loss += loss_fn(output, labels.float().unsqueeze(1))\n",
    "                val_loss /= len(dataloader_val) # average loss\n",
    "            \n",
    "            print(f\"Epoch {epoch:<3}, iter {i:<4} - train_loss {loss.item():.4f}, val_loss {val_loss.item():.4f}\")\n",
    "            counter.append(i)\n",
    "            loss_history_train.append(loss.item())\n",
    "            loss_history_val.append(val_loss.item())\n",
    "            BiLSTM_model.train()\n",
    "\n",
    "# SAVE THE MODEL\n",
    "torch.save(BiLSTM_model.state_dict(), \"models/BiLSTM_Classifier.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHPCAYAAADeeZfZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAABruUlEQVR4nO3dd3hT9eIG8PecjDZt6WKULRRo2VNABKoMUbZylb2HcH+iVxQUvQpX4SquKwoiyhDBgQNwsBRBAQEB2ZtSNpQyWjqTZpzz++Nw0pa20JFmnLyf5+nT9uQk+SYnyXnznYIsyzKIiIiIyOeJni4AEREREbkGgx0RERGRRjDYEREREWkEgx0RERGRRjDYEREREWkEgx0RERGRRjDYEREREWkEgx0RERGRRjDYEREREWkEgx0RkZtNnToVLVq0KNK+sbGxmDNnThmXiIi0gsGOiDRj5cqViI2NxaFDhzxdFI/6+eefsWTJEk8Xg4g8QO/pAhARUeEOHjwInU5XrOusXr0a8fHxGDlyZNkUioi8FoMdEZEXCwgI8HQRAAB2ux2SJMFoNHq6KER0B2yKJSK/c/ToUYwdOxYtW7ZEixYtMGLECOzfvz/PPjabDXPnzkW3bt3QpEkTtG3bFoMGDcK2bduc+1y7dg0vvfQS4uLi0LhxY3To0AH//Oc/cfHixSKVIykpCf/3f/+HFi1a4L777sNbb70Fh8ORZ5/b+9hlZGTgv//9Lzp37ozGjRujXbt2GDVqFI4cOQIAGDZsGP744w9cunQJsbGxiI2NRefOnZ3Xv3HjBl5++WXcf//9aNKkCfr06YNVq1bluc+LFy8iNjYWixYtwpIlS9C1a1c0adIEBw8eRPPmzTFz5sx8j+XKlSto0KABPvnkkyI9diIqG6yxIyK/Eh8fjyFDhiA4OBhjx46FXq/HN998g2HDhuGLL75As2bNAABz587FJ598gieeeAJNmzZFRkYGDh8+jCNHjqB9+/YAgKeffhqnTp3C0KFDUa1aNSQnJ2Pbtm1ITExE9erV71gOh8OBMWPGoGnTpnjhhRewY8cOLF68GDVq1MDgwYMLvd706dPxyy+/YOjQoahTpw5u3ryJPXv2ICEhAY0aNcKECROQnp6OK1eu4KWXXgIABAcHAwAsFguGDRuG8+fPY8iQIahevTrWr1+PqVOnIi0tDSNGjMhzXytXrkR2djb69+8Po9GIqlWromvXrli3bh1eeumlPE3Eq1evhizL6N27d/EPChG5jkxEpBErVqyQY2Ji5IMHDxa6z//93//JjRo1ks+fP+/clpSUJLdo0UIeMmSIc1ufPn3kJ598stDbSU1NlWNiYuSFCxcWu5wvvviiHBMTI8+dOzfP9kcffVR+7LHH8myLiYmRP/zwQ+f/rVq1kl977bU73v6TTz4pd+rUKd/2JUuWyDExMfKPP/7o3Ga1WuUBAwbIzZs3l9PT02VZluULFy7IMTExcsuWLeUbN27kuY2tW7fKMTEx8ubNm/Ns7927tzx06NA7louIyh6bYonIbzgcDmzbtg1du3ZFjRo1nNsrVaqEXr16Yc+ePcjIyAAAhIaGIj4+HmfPni3wtgIDA2EwGLBr1y6kpqaWqDyDBg3K83+rVq3u2owbGhqKAwcOICkpqdj3t2XLFlSsWBG9evVybjMYDBg2bBiysrKwe/fuPPt369YNkZGRebbdf//9qFSpEn7++WfntpMnT+LEiRPo06dPsctERK7FYEdEfiM5ORlmsxm1a9fOd1mdOnUgSRISExMBAM888wzS09Px8MMPo3fv3njrrbdw/Phx5/5GoxGTJ0/Gli1b0L59ewwZMgQLFizAtWvXilSWgICAfKEpLCzsriFx8uTJiI+Px4MPPojHH38cc+bMwYULF4p0n5cuXcI999wDUcz70V+nTh0AwOXLl/NsL6g5WRRF9O7dG7/99hvMZjMAZXqVgIAAPPLII0UqBxGVHQY7IqICtG7dGhs2bMAbb7yBevXq4fvvv0e/fv3w3XffOfcZOXIkfvnlFzz33HMICAjABx98gB49euDo0aN3vf3iTmGi6tGjB3777Te88sorqFSpEhYtWoSePXti8+bNJbq9OwkMDCxw+6OPPoqsrCz89ttvkGUZq1evxoMPPohy5cq5vAxEVDwMdkTkNyIjI2EymXDmzJl8l50+fRqiKKJKlSrObeHh4fjHP/6B//3vf/jjjz8KXAWiZs2aGD16NBYvXozVq1fDZrNh8eLFZfo4KlWqhCFDhmDevHnYuHEjwsPDMX/+fOflgiAUeL1q1arh3LlzkCQpz/bTp08DAKpWrVqk+4+JiUHDhg3x888/4++//8bly5fRt2/fEj4aInIlBjsi8hs6nQ7t27fHxo0b8/Rlu379OlavXo1WrVohJCQEAJCSkpLnusHBwahZsyasVisAwGw2Izs7O88+NWvWRHBwsHMfV3M4HEhPT8+zrXz58qhUqVKe+zSZTPn2A4C4uDhcu3YNa9eudW6z2+1YtmwZgoKC0Lp16yKXpW/fvti2bRs+//xzhIeHIy4urgSPiIhcjdOdEJHmrFixAlu3bs23ffjw4Xj22Wexfft2DB48GIMHD4ZOp8M333wDq9WKKVOmOPft2bMn2rRpg0aNGiE8PByHDh1yTjMCAGfPnsXIkSPxyCOPoG7dutDpdPjtt99w/fp19OzZs0weV2ZmJh544AE8/PDDqF+/PoKCgrB9+3YcOnQIU6dOde7XqFEjrF27Fm+++SaaNGmCoKAgdO7cGQMGDMA333yDqVOn4siRI6hWrRp++eUX7N27Fy+//LIz1BZFr1698M4772DDhg0YNGgQDAZDWTxkIiomBjsi0pyvv/66wO39+vVDvXr18OWXX+K9997DJ598AlmW0bRpU7zzzjvOOewAZaLfTZs2Ydu2bbBarahatSqeffZZjBkzBgBQuXJl9OzZEzt27MBPP/0EnU6H6OhozJ49Gw8//HCZPK7AwEDnJMm//vorZFlGzZo1MX369Dxz3w0ePBjHjh3DypUrsWTJElSrVg2dO3dGYGAgli1bhnfffRerVq1CRkYGateujTfffBP9+vUrVlkqVKiA9u3bY/PmzWyGJfIigizLsqcLQUREvuepp57CyZMnsWHDBk8XhYhuYR87IiIqtqtXr7K2jsgLsSmWiIiK7MKFC9i7dy++//576PV6DBgwwNNFIqJcWGNHRERFtnv3brzwwgu4ePEiZs2ahYoVK3q6SESUC/vYEREREWkEa+yIiIiINILBjoiIiEgjGOyIiIiINIKjYn2MLMuQJKVbpCgKzr/Jc3gcvAOPg3fgcfAePBbeQRSVtZsLW8PZ1RjsfIwkyUhOzoReLyIiIhhpaVmw26W7X5HKBI+Dd+Bx8A48Dt6Dx8I7qMfB4ZCg07kn2LEploiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgj9J4uABFRiWRkQHf2DHRnEmA4dwZITYaxZm2gaQvYGzQCjEZPl5CIyO0Y7IjIawlpqUp4O50A3ZnTzh/xzGnoribl2z/41o9sNMLeqDHszVvC1rwl7M1bwhETC+h0bn8MRETuxGBH2iPLgCQBdjtgt0Ow2wC7Q/nboWyDIAAGA2S9ATDoIev0gMGg/Ig+0EPBZoNgMQNmCwSLGYLFUvD/2dmQTUGQQ0Mhh4VBCg2DHBYOOTRUeaxeQLiZkhPacge4s6chXr9+x+tKkZFw1I6GVKcuAqpVgW3fAej274V48yYM+/bCsG8vTLf2lYOCYG/S7FbQawF7i5Zw1Ip23fE2m6G7fBHixYvQXboI8eIFiJcuQnfxIsSkRMjBwZAqRUGqVBlSVJTyd1RlSJUqKb8rVgICAlxTFiIqnsxMGA4dgP7gftibNoftvvs9XaISY7AjAIDpk49g3PArIEtKKJIkCJIEOBy5timBScizTcrZJsl5twNKgLr9BwIgKJfJBV1e0P4A4FCDmiPnb4cdsNnzhDbBbi/VcyGLohJ6dHrIhlvBT38r9Ol0yja9AbLBAMFgAAKNCHFIkHGH8gs5j1n9KfSxOxxKMDObAUshwc3hKNVjBJSgowS9MMjlQiGF3fo7VPlxXhYWBik0VNkeaIJgzYZgsdwqW06AFMxmCNnZt/63QDBbIGRbcj2G7Jx9LWYIZgvExEsQU1LuWE6pQkU4ouvAUTs670+t2pDDIwAAer2IgIhgZKRkwm5zQDx7BoYD+6Dftxf6/XuhP3gAYmYGDDt3wLBzR85th4bB3qwF7M1bOAOfVL3GreOWuxAShGvXoLuUK6xdunDr90Vl+11CaFFIERE54U8NfJWi8gVBOSwcsFqV5/PW8+p87m8973mOi+XWsXBeZgHUY5JtgRwQCKl8ecgRkZAiIyGXLw8psjykCOVvuVxo/ueEtMXhgHD9OsSrSdBdvQIxKQni1SSIScrfyLYonwHh4bc+KyKUz4aw8Dy/5fBwyCHlvPsLst0O3bGjMOzbA/3+vTDs3QPd8aPKOQ+Ao3oNJO894uFClpwgy7Ls6UJQ0TkcEpKTM6HXi4iICEZKSibsdql0NyrLiGwaC13SFdcU0kvJogjo9UpznCwrtV4uCEieJptMkAMDIQcqvxFogmxS/ofRqJzY01IhpKZCSEuDmJHu6SLn44iqnC+8SWp4Kxd61+vf9f3gcEB3Kl75EN+/F/r9+6A/fBBCdna+XaUKFWBr3hJyZHmIly9Bd/ECxMuXIFitdy2HHBQMR40akKpVh6NaDUjVq8NRrTqkKlUhZGbmnCivXlV+X0tynkCLcvueIuv1Sui7FfjkW6FPKl8ecmTkrW2RECpWROg9VZGaboHDZlfeZzKUL3qynFObfuu3gNu25blc+V9ArlOUXIS/73KZbAwATIFKTbbJpLxvTCbAZPLuMFJMzvdE4g1IlxOdAS3P6y73a/H6NZd9HsqieKuVIPxW4LsV/sLDc8JhxUpwVKkCqXJVSFWqKF/SyuLLgyxDPHdWCXF79yi/Dx1QvjjfxlGlKuzNW8IyeBisD3d3yd2rx8HhkKDTuef1xWDnY8ok2AHQJcRDv3ePEnpEUQlBgqh80Ol0gCjk3X5rP/VHFkRAl/O/8w2qfljn+8m5zPnBXei+ty43GJQmU70O0N9qPtUrP7I+19+373Orpq3AD21ZVppmbTalydamNt2q23JdZrfnXH5rm06WEBKgQ0a6GQ6HckIS7vQ4bn9MBe2v0+UNarcHt1z/IyCg+B+GdjuE9DQIqakQb/1WQl8qxLRcf98KgmooFNNSIZjNt+475/5lkwlyQIASKG/9j4AApby3/x8YCAQGQg5QbkOqWAmO2tFAcHCJXreqEr0fbDbojh+7FfRuhb1jRwqt8ZVFEVLlKkpoq14dUrUaSmirrv6urtSkleTkJMsQbqbknGSvJuWtMbl2NefEnHoz71UFQXlN3P4c33qenX8HBgIB6uvo1vEKDIRsDFBq8ZJvQEy+ATE5GUJycs7fWZnFfzw+Sn29yqagW++1IOVL0q0QiFshUL1cNpmUz53c3Tpu1egr23JdZjRCNugLvuxWywAcklLTmmW+raY+529nbWuufaDuazY7a9PFrEzorl0Fbt4s+uMXBMgVKjpriR1RlZ21xAg0KZ8HqSnKZ8PNm8rnxK3fws2bymeExVKy595kghRVGY4qStBTA5+jSlXn31JU5bsOihKuXYNhf64Qt38vxOTkfPtJ5UJhb94S9patYGvRCvYWLSFVqVqist8Jgx3dVVkFOyoZHgfv4LLjYDZDf/Qw9Pv3QsjIgJQ7uFWu4h39Ei0WCGlpQKAS5GAwlG0zqdkMMSUZwo0bEFOUwKf+XVAY1KXeVL6jqF0LxFzdDEQRgHDrC2KubbddDrGArgqqovxd2GWyfKsrgAWCOcvZfUDrZKMxV7N+rmb+qNv6e1aoqATV0rBYlC+CqTchpN689TtX8Lt589aXmCToEhMhXrlcYPAqjFShYr7wJxuM0B86AMO+PdCdP1fg47c3bgJ7i1shruW9cETXcUsNLYMd3RWDnXfhcfAOPA7ewSePg8OhhBHzrRowsxmCOSun1uzW/4LFApizcmrKzGalpsxmB+w2CDbbrd9KLX/ONrvzsjw1/jZbvtYC6EQlrN+qDVRqBYNu1bqacmoJi/C/GBKMcnVr4aYpFLZgL+8jabFAvJII3ZVEiImXISbe+n0lEbpbv8UriUXrDiEIcNSLyRXiWinTH3loYJIngh0HTxARkf/S6YDgYMjBwdBSLYdeLwIRwZBTMgFvD9mBgZBq1YZUq3bh+8iyUlOceBm6K7nCX9IVCJkZsDdqqoS4Zs2L1C9XyxjsiIiIyLsJAuQKFeCoUAGOJk09XRqvpp0hQERERER+jsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsGOiIiISCMY7IiIiIg0gsHOzaZNm4aOHTsiNjbW00UhIiIijWGwc7PevXtj1apVni4GERERaZDXBrtVq1bh0UcfRZMmTdC2bVuMHTsWFovF5fdz7tw5TJs2DX379kXDhg3Rq1evQvdNSEjAqFGj0Lx5c7Rv3x5vv/02rFZrse6vdevWqFChQmmLTURERJSP3tMFKMjHH3+MBQsWYMKECWjevDlSUlKwY8cOOBwOl99XfHw8Nm/ejGbNmkGSJMiyXOB+qampGDFiBGrVqoU5c+YgKSkJs2bNgsViwbRp01xeLiIiIqLi8rpgd/r0acydOxfz5s3DAw884Nz+8MMPF3qd+Ph4BAQEoGbNmvku27VrF+rXr4/Q0NACr9u5c2d07doVADB16lQcPny4wP2WL1+OzMxMzJ07F+Hh4QAAh8OB1157DePHj0dUVBQee+wxXL58Od91GzdujEWLFhVafiIiIiJX8Lqm2JUrV6J69ep5Qt3dzJs3DyNHjkRiYmKe7bt27cK4ceOwcuXKQq8rikV7CrZs2YJ27do5Qx0AdO/eHZIkYdu2bQCU5uOdO3fm+2GoIyIiInfwumB34MABxMTEYN68eWjXrh0aN26MgQMH4sCBA4VeZ8aMGYiIiMDIkSNx48YNAMDBgwcxYcIE9OjRAyNGjCh1uU6fPo3o6Og820JDQ1GxYkWcPn261LdPREREVFpeF+yuXbuGP//8Ez/++COmT5+Ojz76CIIgYPTo0c7QdruQkBAsXLgQBoMBo0ePxu7duzFu3Dh06NABM2fOhCAIpS5XWlpagc25YWFhSE1NLfLtTJ06FXFxcQCAuLg4TJkypdRlIyIiIgK8sI+dLMvIysrCBx98gPr16wMAmjVrhs6dO+OLL77Av/71rwKvFxERgcWLF2PIkCEYOnQoOnbsiPfeew86nc6dxb+rWbNmeboIREREpFFeV2MXGhqK8PBwZ6gDgPDwcDRs2BCnTp2643WtViuys7MhiiIsFotLR9GGhoYiPT093/bU1FSEhYW57H6IiIiISsrrgl3dunULvSw7O7vQy65evYpRo0ahatWqWLFiBc6ePYuJEycWe565wkRHR+frS5eeno5r167l63tHRERE5AleF+w6deqEmzdv4tixY85tKSkpOHLkCBo1alTgdVJSUjB69GiEhITg008/RcOGDfHZZ5/h0KFDmDx5sktq7uLi4rB9+3akpaU5t61fvx6iKKJ9+/alvn0iIiKi0vK6YNe1a1c0adIEzzzzDNauXYuNGzdiwoQJMBqNGDx4cIHXmT59OiRJwqJFi5wDHOrVq4dFixZh+/btWLp0aaH3ZzabsX79eqxfvx6XLl1CRkaG8//k5GTnfgMHDkRwcDCeeuop/Pnnn1ixYgXefvttDBw4EFFRUa59EoiIiIhKQJALW2rBg5KTk/Hmm2/i999/h81mw7333ouXXnqp0GbaCxcuwGg0Fhiwjh49itq1a8NkMhV43YsXL6JLly4FXrZ06VK0bdvW+X9CQgJmzJiBffv2ITg4GH379sWkSZNgNBpL8ChLxuGQkJycCb1eREREMFJSMmG3S267f8qLx8E78Dh4Bx4H78Fj4R3U4+BwSNDp3FOX5pXBjgrHYOddeBy8A4+Dd+Bx8B48Ft7BE8HO65piiYiIiKhkGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiINILBjoiIiEgjGOyIiIiIANy4ISA11dOlKB0GOyIiIvJ76elAu3bB6NMnyNNFKRUGOyIiIvJ7Bw/qcPOmgMxMwdNFKRUGOyIiIvJ7x44pkahhQ4eHS1I6DHZERETk944eVYOd5OGSlA6DHREREfm9o0d1ABjsiIiIiHyawwEcP86mWCIiIiKfd+6cgKwsAYGBMmrXlj1dnFJhsCMiIiK/pjbDxsZK0Ok8XJhSYrAjIiIiv6aVgRMAgx0RERH5uZxg59v96wAGOyIiIvJzWhkRCzDYERERkR/LyFAGTwBAgwYMdkREREQ+68QJEbIsoFIlCRUq+PaIWADQl+bKly9fxuXLl3Hvvfc6tx0/fhyLFy+G1WpFr1690LVr11IXkoiIiKgsaKkZFihlsJs5cyaysrKwZMkSAMD169cxfPhw2Gw2BAcH45dffsEHH3yAbt26uaKsRERERC6lpRGxQCmbYg8ePIj777/f+f8PP/wAi8WCH3/8EVu2bEG7du2wePHiUheSiIiIqCxoaUQsUMpgl5qaivLlyzv//+OPP9C6dWvUrFkToijioYcewunTp0tdSCIiIiJXk2Xg2DGlKVYLAyeAUga7yMhIXL58GQCQlpaG/fv3o2PHjs7LHQ4H7HZ76UpIREREVAYSEwXcvClAp5MRE6ONYFeqPnb3338/li1bhpCQEOzcuROyLKNLly7Oy0+dOoUqVaqUupBERERErqY2w9arJyEgwMOFcZFSBbvnn38eZ86cwVtvvQWDwYAXXngBNWrUAABYrVasW7cOvXv3dklBiYiIiFxJayNigVIGuwoVKmD58uVIT09HQEAAjEaj8zJJkvD555+jcuXKpS4kERERkaupNXZa6V8HlDLYqcqVK5dvW2BgIOrXr++KmyciIiJyuWPHtDUiFijl4IkdO3Zg4cKFebZ9//33ePDBB3H//ffjjTfegMOhnSeLiIiItCE7G4iP19YcdkApg92cOXNw/Phx5/8nTpzA9OnTERkZiTZt2mDZsmVYtGhRqQtJRERE5Erx8SLsdgFhYTKqVvX9pcRUpQp2CQkJaNy4sfP/H3/8ESEhIfjyyy8xe/ZsPPHEE/jxxx9LXUgiIiIiV8o9MbEgeLgwLlSqYGc2mxESEuL8f+vWrejQoQNMJhMAoEmTJs557oiIiIi8hdYmJlaVKthVqVIFhw4dAgCcO3cO8fHx6NChg/Py1NTUPCNliYiIiLyB1taIVZVqVGzv3r3x0UcfISkpCadOnUJYWFieCYqPHDmCWrVqlbaMRERERC6ltTViVaUKdhMmTIDNZsPmzZtRpUoVzJo1C6GhoQCAmzdvYteuXRg+fLhLCkpERETkCjduCEhKUoJd/fqsscu5sl6PSZMmYdKkSfkuCw8Px7Zt20pz80REREQup85fV6uWhFxDBTTBJRMUA0BmZiauXLkCAKhcuTKCg4NdddNERERELpOz4oS2mmEBFwS7gwcP4p133sHevXshSUp1piiKaNWqFaZMmYImTZqUupBERERErqLVgRNAKYPdgQMHMGzYMBgMBjz++OOoU6cOAGV+uzVr1mDo0KFYtmwZmjZt6pLCEhEREZXW0aPKVCcMdrd5//33ERUVha+++goVK1bMc9nTTz+NQYMG4f3338dnn31WqkISERERuYLDARw/rtTYNWqkvabYUs1jd+DAAQwYMCBfqAOAChUqoH///ti/f39p7oKIiIjIZc6eFWCxCDCZZNxzj3aWElOVKtiJogiHo/C0K0kSRLFUd0FERETkMmozbP36EnQ6DxemDJQqdbVo0QJffvklLl26lO+yy5cv46uvvkLLli1LcxdEREQukZamNMORfztyRJsTE6tK1cfuueeew5AhQ9C9e3c89NBDzlUmzpw5g40bN0IURTz//POuKCcREVGJ7dsnonfvIAwfbsMbb2R7ujjkQVoeEQuUMtg1bNgQ3333Hd5//31s2rQJZrMZAGAymdCxY0dMnDgRERERLikoERFRSX31lQFWq4B16/QMdn7u2DHtjogFXDCPXd26dfHRRx9BkiQkJycDACIjIyGKIj7++GN8+OGHOHbsWKkLSkREVBKSBKxfr5zuLl0SkZwMREZ6uFDkERkZwLlz2p2cGChlH7s8NySKqFChAipUqMABE0RE5DX27BGd64ICwOHDGuwxT0WiLiVWubKk2XDPBEZERJq2dq0hz/+HD/PU56+0PDGxiq9uIiLSLFkG1q5VmmEbN1aa3g4dYo2dv8oZOKHNZliAwY6IiDTs+HERZ86ICAiQ8cwzVgCssfNnalNsgwbarbEr9uCJI0eOFHnfq1evFvfmiYiIXEatrXvgAQfuu0+ppYmPF2E2AyaTJ0tG7ibL/tEUW+xg949//AOCIBRpX1mWi7wvEXk/WQb4liZfoga7Hj1siIqSUaGChOvXRRw7JqJlS+2e3Cm/S5cEpKUJ0Otl1Kun3WNf7GD35ptvlkU5iMiLyTIweXIA1q3TY/36LNSsqb31FctSWhoQHAxNLl/kzc6fF3DokA6iKKNbNwcEAWjcWMIff4g4fFjHYOdn1P519epJMBo9XJgyVOxg99hjj5VFOYjIiy1aZMCyZcon4bp1eowfb/NwiXzH7t0i+vYNwtixNrz+OifGdad165RT3H33OVChgvJlpHFjB/74Q49Dh9jPzt9ofWJiFV/ZRHRHe/aImD49wPn/33+z2qk4li83wG4XsHKlHjIrOt0qpxnW7tzWpIlyUudcdv5HrbHT8sAJgMGOiO4gORkYN84Em01ATIzS8ZzBrugkCfjlFyVcXL0q4swZdlB0l2vXBPz1l/JazR3sGjdWTurHjolwaHfGCyqAGuwaNdL2gWewI6ICSRLw1FMmXLwoIjpawvffmyGKMi5dEpGYyIBSFPv2ibh6NedjdseOUq/iSEX0yy96yLKAZs0cqF49p6o0OlpCUJCMrCwBCQk8BfoLiwU4dUqdw441dkTkhz780IiNG/UIDJSxaJEZlSvLzg9E1toVjVpbJwhKsNi+nc+buxTUDAsoA1jU1zHns/Mf8fEiHA4BEREyKlfWdp8IvqqJKJ8//9Rh1ixlsMSsWRY0aqScCO+9V2nC2LWLAaUo1IXn+/dXwoXaNEhlKz0d2LIlfzOsiitQ+J/cK05ofcomBjsiyiMpScD48YGQJAEDB9oweHDOiVENdqyxu7szZwQcP66DTidj6tRs6HQyLlwQceGCxs8qXmDjRj2sVgF16kiIicnf7JYzgIKnQH+hTkys9YETAIMdEeVitwNPPhmIa9dENGjgwKxZljyXq8Hu0CER2Zy5445+/VWprbv/fgeqVZPRrJlyQmGtXdnLPSlxQbUzao3d4cMiRyr7iZwaOwY7IvIjs2YZsWOHHiEhMhYvNiMoKO/ltWvLKF9egtUq4OBBfnzcidoM+/DDSo2nupzVjh0MdmUpOxv47beC+9epGjSQoNPJuHFDxJUrrEH1B7mbYrWOn8xEBAD49VcdPvxQma9u9mwL6tTJX5UhCEDr1myOvZuUlJyaOTXYtWun/ObI2LK1dasOGRkCKleW0KJFwbUzgYFwNtFyomLtu3ZNwLVrIgRBRmwsa+zIxaZNm4aOHTsiNjbW00Uhcjp/XsDEicqK6GPHWtGnT8E1HQBw770cGXs3Gzfq4XAIaNDAgXvuUQJy27YOCIKMhAQRSUmsJSorajNs9+52iHc4w6kDgjiAQvuOHVNeCLVqyQgO9nBh3IDBzs169+6NVatWeboYRE7Z2cDYsSbcvCmgVSsH/vOfO3eeU/vZ7d6tY/+kQqjNsI88khOQw8Nz+vfs3MkwURYcjpznvrBmWFWTJjn97Ejb/KkZFvDyYJeZmYm4uDjExsbi0KFDZXIf586dw7Rp09C3b180bNgQvXr1KnTfhIQEjBo1Cs2bN0f79u3x9ttvw2q1Fuv+WrdujQoVKpS22EQuM316APbv1yEiQsaCBea7Lo7drJkDOp2MK1dEXLrEmqfbZWcDmzbl7V+natdOObFwPruysXu3DteviwgPl3H//Xc+iasrULDGTvvUEbH+MHAC8PJgN2/ePDjKeM2X+Ph4bN68Gffccw/q1KlT6H6pqakYMWIEbDYb5syZg0mTJuHbb7/FrFmzyrR8RGVp1So9Fi9Wkty8eeY8M/QXJjg4pxmLzbH5bd+u9PGKipLQvHneE4ka7DiAomysWaME6ocessNguPO+6sjY8+dFpKaWdcnIk/xpRCzgxcEuISEBX331FZ5++um77hsfH4/z588XeNmuXbuQlpZW6HU7d+6MzZs348MPP0SjRo0K3W/58uXIzMzE3Llz0bFjRzz++OOYMmUKli9fjqSkJADAY489hrZt2+b7GTNmzF0fA5G7xceLmDQpEAAwaVI2unQp+pcozmdXOHW1iW7d8vfxattWed6OHdMhJcXdJdM2WQbWrStaMywAREQA1asrJ/ojR/g61iq7HTh5kk2xXmHmzJkYOHAgateufdd9582bh5EjRyIxMTHP9l27dmHcuHFYuXJlodcV79S7NpctW7agXbt2CA8Pd27r3r07JEnCtm3bAACrVq3Czp078/0sWrSoSPdB5C6ZmcCYMYHIyhLQoYMdL7xQ3C4FDHYFkeWcYJe7f52qUiUZ9eopzx372bnW4cMizp8XYTLJ6NTp7sEOyL0ChdeeCr3Crl0ifvzRN0dznzkjwmIREBQkOwcyaZ1XvprXr1+PkydP4qmnnirS/jNmzEBERARGjhyJGzduAAAOHjyICRMmoEePHhgxYkSpy3T69GlER0fn2RYaGoqKFSvi9OnTpb59IneRZeCFFwJx/LgOlSpJ+PhjC3TFzBi5Jyo2m8ugkD7q8GERly6JCAqS0aFDwbUD6nx227f75onSW6mjYR980J5v/sXC5KxAwZBdGFkGRo0yYdw4k3N0qS9Rm2EbNJDuOEpaS7zuYZrNZsyaNQuTJk1CSEhIka4TEhKChQsXwmAwYPTo0di9ezfGjRuHDh06YObMmRBcsDBcWloaQkND820PCwtDajE6aEydOhVxcXEAgLi4OEyZMqXUZSMqji++MOC77wwQRRmffmpBVFTxv8XWrCmjYkUJNpuAAwd4UlSpIzIffNAOk6ngfdR+dlyBwrVyVpsoWm0dkHsAhdedCr3G9evKHHAAsGeP771m/W1ELAB43VfGjz/+GOXLl8c//vGPYl0vIiICixcvxpAhQzB06FB07NgR7733HnTFrYooYxxsQZ506JCIl19WJiF++WXrXUcOFkYQlFq7detE/P236KyF8ncFTXNyOzXYHTwoIiMDKOL3V7qDM2cEHDumrMvbrVvRg5065cnJk8oSeQEBZVVC35WQkBN69+8XMXSoBwtTAmoto78MnAC8rMbu0qVLWLx4MZ555hmkp6cjLS0NWVlZAICsrCxkZmbe8fpWqxXZ2dkQRREWi8WlI2pDQ0ORnp6eb3tqairCwsJcdj/kW2QZOH9emT/L26WmAmPGmJCdLaBbNzsmTixev7rbcaLivC5dEnDokA6iKKNr18JfENWqyahZU4IkCdi1i8+dK6i1dfff70BERNGvV62ajPBwGXa7gBMnvOp06DVOn85p8fLF2nl1qpMGDRjsPOLixYuw2Wx48skn0bp1a7Ru3RoTJkwAAAwfPhyjRo0q9LpXr17FqFGjULVqVaxYsQJnz57FxIkTiz3PXGGio6Pz9aVLT0/HtWvX8vW9I//x738bcc89QO3aQXjiCRPeeceIzZt1yMjwdMnykmXgmWcCcfasiBo1JMyZYy51f5PcAyg4UXHOoInWrR2oUOHOTwinPXGttWuVuU2K0wwLKDXPnKj4znLX2B09qtRs+oq0NGU6GwBo0MAHvn27iFc1xTZo0ABLly7Ns+3YsWN488038dprr6FJkyYFXi8lJQWjR49GSEgIPv30U4SGhuKzzz7D0KFDMXnyZLz//vulbpKNi4vD/Pnz8/S1W79+PURRRPv27Ut12+Sbfv5Zj/nzlRNKRoaAzZv12LxZeUuJooxGjSS0aeNw/lSr5rn0M3++AevWGWA0yli40FysWo3CNGvmgF4v4+pVERcuCKhZ07/TndoMe/ukxAVp186Ob74xMNi5QFKSgL//Vk7exQ12gNLPbutWdaLi4l9f63IHO5tNwLFjYr75Gb3VsWPK+6tqVckln3m+wquCXWhoKNq2bVvgZY0aNSp0nrnp06dDkiQsWrTIGbrq1auHRYsWYeTIkVi6dGmhtX1msxmbN28GoDQFZ2RkYP369QCANm3aIDIyEgAwcOBALFu2DE899RTGjx+PpKQkvP322xg4cCCioqJK9bjJ95w7JzjngJsyBejd24zt25Wmtd27dbhwQcShQzocOqSDOttNtWoS2rZ1oHVrJeg1bCgVezRqSezcqcOMGUrnoddfzy50YfTiMpmUUYX79unw99861KzpvyfF9HRg2zblYN6pf51KrbHbt08HsxmFDrSgu1u/Xg9ZFtCypQNVqhT/ywWnPLmz06eV5yUoSEZWloB9+3Q+E+z8bWJilVcFu5KaMmUKjEajM4SpGjdujKVLl95xLrwbN27gX//6V55t6v9Lly51Bs2wsDB8/vnnmDFjBp566ikEBwfj8ccfx6RJk1z8aMjbWa3A+PEmpKUJaN3agf/+V4eMDAn160sYPdoGAEhMVEKe+qNOg7FypYiVK5VavpAQGa1a5dTotWrlcHlH+uvXBTz5ZCDsdgGPPWbDqFE2l97+vfc6sG+fEmb79fPfYPf773rYbALq1nWgbt27h4tatWRUrizhyhURe/fq0L69/zQTuVpJRsPmpk55cuSIDpIEv5kSoygcDmUeOED5wrJypQEHDvjOE5QzcMK/3l9eH+zatm2LEydO3HGfGjVqFHpZw4YN73jd6tWr3/X2VXXq1MGSJUuKtC9p1xtvBGDvXh3CwmQsXJgNgyH/pFlVqsjo29eOvn2Vk01GBrB3b07Q+/tvXaHNt/XqSTAYAL1ehk4H6PWAwYBbf8vQ6+Hcrvzcvk12XvbllwYkJoqoW9eB996zwAUz/+Rx770OLFjAARTqigcPP1y0E4ggKLV2q1aJ2LGDwa6k0tKAP/9UXns9epTsS0vduhICA2VkZgo4e1ZAdLR/dynI7dIlAdnZAoxGGT17KsFu/37fea/nnsPOn3h9sCPyJr/9psO8ecraqh98YEGNGkU7CYSEAHFxDsTFKSdwh0P5NqkGvdubb13JZJKxaJGlTKbVUCcqPnJERFYWijwxrJbYbMBvvxW9f53qvvscWLWK/exKY8MGpaY0JqZoNaUF0euVE/++fTocPqxDdLT/1jzfTu1fV7u2hFatlPf6iRO+8V6X5ZwRsWyKJaICXb4sYOJEpV/d2LHWW00/JWuW0OmUTtuNG+dvvr18WYDdLsDhUNY5tNtx62/B+f/t23Lvm3s/ABg3zlpm31irV5cRFSUhKUnEgQM6Z98xf7Jrlw6pqQLKl5ecI4WLQp1D8O+/dbBaAaOxrEqoXaVthlU1bqx0KTh0SESfPq4omTao/euioyVUqaJMSn7tmogjR0S0bu3dYenCBQEZGQIMBhl163p3WV2NwY6oCOx24J//DERysoimTR2YPt31Y/7V5ltfIgjK9B6rV4vYvds/g506GvahhxzFGgwTEyOhfHkJN26IOHDA+0+U3sZsBjZudFWw49JiBVFr7OrUkSAIQPPmEjZsUL7EefvrVe1fFxOjdG3xJ77TC5LIg957z4gdO/QIDpbx6admzlCfi9ocq0454U9kuXjTnOQmCEDbtup8dvyOXVxbtuiQlSWgWjUJzZqVLmSoc9lxZGxeOcFOaeZu1kx5nnyhn50/Tkys4quYvN5PP+nxyisBSE72zP1v3arD//6ntJO9956FnatvkxPs/G+i4hMnRJw7JyIgQMYDDxS/1ogTFZecOilx9+72Ug8KatBAgiAoczImJbl4hJEPy11jBwDNmyuvV18YGeuPa8SqvP/okN+SZeB//zNi7FgTPv3UiB49gvMsb+MOV68K+Oc/AyHLAoYMsfr1lB6FadpUgsEg4/p1EWfP+tdJUV1tIi6uZFPVqP3sdu7U+cSydN7Cbgd++UUdDVv692RwMJz9sI4c4WkRALKzlX5qgNLHDoCzZvTkSdHrVte5nb/OYQcw2JGXcjiAqVMDMGuW0uYZHi7j9GkRPXsGYdcu97xsJQmYODEQV6+KiI114L//9aG1dNwoMFAJd4D/TXtS0mZYVcOGEsqVk5GRITBQFMPOnTokJ4uIjJRw332uScRqPztXj0r3VWfPipBlAeXKyahYUamKj4qSUaWKBFkWvLo/otmcU9vYqBGDHZHHWSzA2LGB+OwzIwRBxhtvWLB1ayaaN3fgxg0R//hHEH76qez7JM2da8Qff+hhMslYsMDi9cP7PSl3c6y/SEoSsGeP8ni7dStZsNPpcvez85/nrrTU0bDdujmgd9FHQc4ACp4WgfwDJ1Q5/ey893mKjxchScpI9UqV/Kx/CBjsyMukpgIDBpiwZo2yrumCBRaMHWtDVJSMVauy8MgjNmRnCxg71oQ5c4xl1qdr504d3nxT6Vf3xhvZqF/f/771FYc/BrsNG5RE0bKlA5Url/yFqNY4bd/uP89dachyzoTQJZ2UuCA5S4vxOAA5wU5thlWpSxJ68wCK3BMTu3pSdl/AYEdeIzFRQJ8+QdixQ49y5WQsX25Gnz45NSHBwcBnn1kwbpwVADBjRgCmTAlwztfmKikpwIQJgXA4BPTrZ8Pgwa5dhkuL1Pnbjh4VkZnp4cK4SWmbYVX3369cf+dOZUkrurODB0VcvCgiKEjGAw+4rmOiurTY6dPe33/MHdT+zOrACZUvjIw9csQ/JyZWMdiRVzh5UkSPHkE4dkyHqCgJP/6YhQ4d8n9o63TAf/+bjZkzLRAEGUuXGjF0qMllH8SyDPzrX4G4dElE7doS3n3X9ctwaVHVqjKqVpXgcAhe/YHvKpmZynQbQOmDXdOmEoKCZCQnizh5kh/Jd6M2w3bubIfJ5LrbrVBB6T8G5AQDf3b7iFiVOoDi9GkRqaluL1aR+POIWIDBjrzArl0ievUKwqVLIurUkbBmTZazv0thnnzShiVLLDCZZGzapEfv3kFITCx9AluwwID169VmYHOZLMOlVWpz7O7d2j8pbtmih8UioGZNqdTzZBmNcC7XxH52d+eq1SYKwn52OQoLduXLy6hZU9l28KB3vl7VyYlZY0fkAb/8osMTTwTh5k0BrVo5sHp1FmrWLFp/pe7d7fjhhyxUrCjhyBEdHnkkqFQfyPv3i3jtNWUU7muvZTtHelLR+FM/O7UZ9pFHSj+HGsD57IoqIUHAiRM66PUyHnrI9cFOnajY34NdWhpw7VrBfewA726OvXpVwPXrIkRRRkyMf36G+/erlzzqiy8MGDHCBLNZQNeudnz/fRbKly9eJ/QWLSSsW5eFmBgHEhNF9O4dhE2biv9hk54OPPmkCTabgB49bM71W6no1GC3Z4+o6YmKHQ5gwwbXNMOq1Pnsduzwv0mei2PNGmVS4g4dHAgLc/3tq1Nj+PsACnWN2EqVJJQrl/9ytTnWGycqVptha9eW/XYmA+87KqR56sTDzz0XCEkSMGiQDZ9/bkZwcMlur2ZNGWvWZKFDBzsyMwUMGWLC0qVFXxxQloHnnw/E2bMiatSQMHs2+9WVRJMmEoxGGTduiDhzRrtP4J49Iq5fFxEWJrtsDrUWLRwwGmUkJWn7uSutnNGwZTNRuFpjd/y4CJsff7crrBlWpa5A4Y01dv7evw5gsCM3cziAF17ImXh40qRszJ5tKfUizWFhwPLlZvTvb4PDIWDy5EC8/rqxSKMMv/jCgB9+MECnkzF/vhnh4aUri78KCMj5Jq/lfnbqahNduthdtri4yaSEOwD46y/tPnelkZiozBsoCDK6dy+bYHfPPTLKlZNhtQp+PZDlbsGuaVPltXr+vOixpR4Lo64R66/96wAGO3IjiwUYMyYQn3+uTDz85psWvPSS1WW1Y0YjMGeOBS+8oKwQMXduAJ58MhAWS+HXOXpUxL//rYTMl1+2onVr//0wcAV/6GeXu3+dK6n97LZvL/vJt32RWlvXqpWEqKiyaa8WhNzz2fnv6VFtii2ofx0AhIcDtWurzbHe9V7394ETAIMducnNm0D//iasXauMOF240IIxY1zf1iEIwOTJVsyda4bBIOOnnwz4xz+CcONG/vSYmYlbwU9A5852PPWU1eXl8TdaHxmbkCAgPl4Hg0FG585lE+xYY1ewnNGwZdtGqs5n589TnuTU2BUeoNUaZm8KdnY7cOKEOjkxm2KJyszlywL69g3CX38pEw9/840ZvXuXTVOKqn9/O7791oywMBm7d+vQo0eQc8JN1csvB+LkSWXevLlzLRD5big1daLi48e1Ocmr2gx7//0OhIa69rZbt3ZAp5Nx/ryIixfZzy63mzdzVuYoq/51Kn+vsZPluzfFAjkjY/ft857nKSFBhNUqIDhYLvLsClrkPUeENOnECRE9e+ZMPPzTT1lo394936Tat3dgzZos1Kwp4cwZEd27BztrQ777To+vvzZAFGXMn29BhQr++yHgSpUry6heXYIkCdi713u+ybtKWTXDAkBICJxT7LDWLq9ff9XDbhfQoIED0dFl+17NmcvOP0coX70qICNDgCjKuOeewoNd8+be1xSbeykxf/6i7scPncrarl3K9COXLomoW9eBtWuznNMJuEtMjIS1a7PQsqUDKSkCHn/chLlzDZgyJRAA8NxzVrcFTX+h1X52N24I2LVLeUzdupVNrZE6ypbz2eWlNsOW1aCJ3GJiJBgMMtLSBJw/7381p2ptXY0aMgICCt+vSRMHBEHG5csikpK843niiFgFgx2VifXrdXj88bwTD9eo4Zmvv5UqyVi5Mgs9ethgtQp4/fVAZGUJaN/ejuefZ786V1ObY7UW7H77TQdJEtCokaPMXsvqurEMdjmysoDff1eCXc+eZR/sjEagfv2cWjt/U5RmWECpYa5XT12BwjuixLFjHBELMNiRi8kyMG+eASNHmmCxCHjoIWXi4chIz5YrKAhYtMiC8eOVIFe+vISPP7ZA53+f22UuZ6JibS1qX5bNsKq2bZVakFOndLh61TtqQTztjz/0MJsF1Kgh3XWpQVdR57Pzx352RQ12QM70Rt4yn11OjZ2GPnhKwP9etVRm0tKA0aMD8Z//KBMPDx5sLdXEw66m0wEzZmRjzZpMbNqUhcqV/bADjRs0aiQhMFBGSorgPEn4Ooslp9aoLINdeDica8/u3OkdJ0tPW7MmZ1Jid00cnrufnb9RB5kVNtVJbupExd7Qzy41Fbh4kSNiAQY7cpEjR0Q89FAw1qwxwGCQ8dZbFrz/fjb0XjglV+vWEqpUYagrK0Zjzoi5v//WxkfMtm06ZGUJqFJFKvM1hHPms/P8ydLTbDZl4ARQ9qNhc8sJdtp4/RZH8Wrs1BUoPL+MoNoMW726VCbLzfkS/3vVkst9840ePXoE4cwZEdWrS/j55yyMGmXjslx+7N57lZOCVvrZqZPjPvxw2dca5V431t9t365DaqqAChUktGnjvlqYRo2U+7p8WSxwDkytstuBs2eLHuwaN5YgijKuXhVx5Ypnnyc2w+ZgsKMSs1iA558PwNNPm2A2C+jUyY4NG7LQsiXfWP5OSyNjJSmn1qgsm2FVbdsqz92xYyJSUsr87ryaOhr24Yftbu0PW65czsoK/lRrd+GCAJtNQECAjGrV7l4FFxQExMZ6Rz+7nKlO/LsZFmCwoxI6d05A795BWLZMWR5sypRsfPWVGeXLs4mTcoLd8eMi0tI8XJhSOnhQxJUrIoKDZbdMjVOpkoy6dR2Q5ZzpVfxNSgowc6YRX3+tLMbrzmZYlT8OoMi9lFhR54HLWYHCs88T14jN4T+vWHKZDRt0eOihYBw4oENEhIyvvzZjyhQrR5iSU1SUjJo1Jciy709UrI6G7dzZfsd5vVzJX9eNTU8H3n3XiHvvDcGHHwbAYhHwwAN2PPCA+2th/HEARXH616m8YWSsJHGN2NwY7KjIHA7gzTeNGDJEmZ+uZUsHNm7MROfOrPqm/LSybqwa7B5+2H21Rv62bmxmJjBnjhLo3n47AOnpynyBy5Zl4dtvzTAa3V8mtcbOn5piSxLs1JGxnhxAcf68gMxMAUajXKyya5X/vGKpVK5fF9C/vwnvv69UWYwebcWPP2ahenU2vVLBtDBR8fnzAo4e1UEUZXTt6v5gd/CgNtfcVVkswIIFBrRpE4wZMwKQkiKgXj0HFi40Y+PGLDz8sMNjg7DUGrtTp0RkZXmmDO5WkmDXsKGyUkdysogLFzxzsNQRsbGxklfOxOBuDHZ0V7t3i+jSJQhbt+oRFCTj44/NmDUr223NUuSbtDBRsTpoom1bh1sn2a5WTWnKdjgEn6/xLIjNBixdasB99wXj3/8OxLVrImrWlDBnjhlbtmShTx+7x9f6jIqSUbGisu6x2syndbn72BVVQEDO3Iuems8u9xqxxGBHdyDLyrfpvn2DkJiorPe6fn0W/vEP93dkJt/TsKEEk0lZczM+3jc/atRpTtwxGvZ2Wlw31uEAvv1Wj/vvD8bkyYG4fFlE1aoS3n3Xgh07MjFggHtHv96NWmt36JAXFaqMmM05E/zWqVO8lpjc89l5AteIzcs3P22pzGVkAE8+GYh//zsQdruAvn1t+PXXLOcaikR3YzDk9L/xxebY1NScUOXO/nUqLa0bK0nAjz/qERcXhIkTTTh3TkTFihL++18L/vorE8OH22AweLqU+flTP7szZ5THGB4uIzKyeMGueXPPDqDgiNi8tP9qpWI7flxEt25B+PFHA/R6GTNnWvDppxaEhHi6ZORrcvrZ+d5HzaZNetjtAmJiHIiOdn9fUrXGbt8+Hcxmt9+9S8gy8MsvOnTpEoRx40yIj1dG0r/6ajZ27crEuHE2BAZ6upSFa9LEf0bG5u5fV9x+jbmXFnP3AIqsLODMGaXADHYKdjOkPFas0OP55wORlSWgcmUJCxea0aYN3yxUMr48MlYdDeuJZlgAqF1bRlSUhKQkEfv26ZwrUvgCWQb++EOHWbMCnNPdlCsnY8IEKyZMsKJcOQ8XsIgaN1ae86NHRdjt0HTH/JL0r1PFxkoICFC6XZw5I7j1i9DJkyIkSVmdpFIlDuYDWGNHt2RnA1OnBuCf/zQhK0tAx452bNyYxVBHpdKqlfL6OXlSh5s3PVuW4rBagY0b3T/NSW6C4Jvrxm7dCvTuHYj+/YOwd68OQUEynnkmG7t3Z2DKFN8JdYASroOCZFgsgrNGS6tKMiJWZTQCjRp5ZgAFB07kp+1XKhWJLAMjR5qweLEyWdSkSdn49lszKlbktx8qnYoVZdSqpXzg+tJExX/9pUNamlIL4Mkl8tRg5yv97JYs0SMuTgmiAQEyxo+3YteuTLzyitWto4pdRRRzAovWV6AoTbADcs9n5+5gx/51t9P2K5WKLDFRQESEjC++yMJLL3EVCXIdX2yOVZthu3Xz7ChNNdj9/bcOVqvnylEUGRnAf/+rfDkcMMCGnTszMWNGts83j+UMoPCd129JnD6t9FMrSVMskLufnftihSwDW7cqx6VRI9/pqlDWNNxjgIpKEID167MgivDIDO+kbffe68D33xt8YmTspUsCli0zYPlyZYimp/rXqWJiJERGSkhOFnHwoIh77/XeWonFi424cUNAnTrAnDlWAL4d6FQ5U55otx4kJQW4cUN5fLVrl+w1pi4tduCADg4H3PKFaO9eEceO6RAYKHv8vepNtPtKpWIJDGSoo7Khjozdu1f5wPc2kgRs2qTD8OGBaNUqGP/7XwAyMgQ0aeLwyBqluYmiMjky4N3rxmZkAPPmKWH41Ve1NchArbE7csRzS2aVNXXgRJUqUolnP6hXT0JQkIzMTPf1R/ziC+U117u3HeHhbrlLn8BgR0RlqkED5QM/PV3AiRPe85GTnAx89JGy+sHAgUFYv94ASRLQoYMdixaZsX59FkwmT5cSztGw3rxu7KJFRiQni6hTR8KQIZ4ujWspy1QpS2Zdvuyh9c3KWGn71wFKmFdHEbtjouL0dGDVKiXYDRtmK/P78yXe8ylLRJqk1wMtW3rHRMWyDOzZI+LppwPRrFkIXnstEGfPiggNlTFunBV//pmJlSvN6N3b7jUT5qr97Hbu9M4az/R0YN48pbp/8mSbpmrrAKU1o149bTfHlmaqk9zUiYrdMTJ25UoDsrKUeSbVWm1SaPNVSkReRR1A4algl5mpNNs89FAQuncPxjffGJCdrTS3/u9/Fhw4kIH//jcbMTHe14etUSMJ5copNZ7q1A7eZOFCI1JSBNSpI2l2uUGtT1Tsiho7IPfSYmX/PC1bpnzzGjrUVuwJlbVOY9+tiMgb5QQ79waT+HgRn3+uDIZIS1M+/QMCZPTta8eoUVa0bFn8WfbdTadT+tn99pse27frnCHDG6SlAR9/rNTWPf98tuZq61SNGzvw7bcGzdbYuSrYqTV2hw+X7YTOBw6IOHhQB6NRRv/+bIa9nUbfhkTkTdSJik+d0iElBYiIKLv7stmU6UqWLDFg69acj7h77pEwcqQVgwbZfG5OtfvuU4Ldjh06jB/vPSeyBQuMuHlTQL16Djz2mB1abQRSw/SRI9qrsZPlnKbY0ga7OnUkhITIyMgQcPKkWGZzy6m1db162X3uvewODHZEVObKl5dRp46EhAQRe/bo0LWr6/vEXLwIfPihAUuX6pGUpJyoRFFGt252jBxpw4MPOiD6aO5o184OIAB//aWsxekNtYypqcD8+WptnbbnvlQHBZw/L+LmTWhqBOaVKwKysgTodDJq1izdsF9RVJpjt23T48CBsgl2GRnAihUcNHEnPvoxR0S+pqwmKrbbgRkzDKhVC3jnHSOSkkRUrChh0qRs/P13JpYutaBzZ98NdYAyR5jJpIzMPHnSOx7Ip58akZqqdF7v21ebfetUYWFAzZra7GenNsPec4/skgFD6nx2ZdXP7ocfDMjMFBAdLfnU+snu5B2fEESkeWUxgCIxUUC/fia8/74RDocyNcinn5qxb18mXnrJiurVtTHxmNGY8/x5w7qxqanAJ5+oI2G1XVunUmvtDh/W1mnTVf3rVDkrUJTNi0Kdu27oUKtX1Fx7I229QonIa6nBxFUTFW/apEPnzkH46y89QkJkfPMNsHq1BY8+atfkZNvqtCfeMJ/d/PlGpKUJqF/fgT59tF1bp8pZgcLzz78rqcGutFOdqNSRsUeOiC5fBu/wYRF79+pgMMgYMMA/XnclwWBHRG5Rv77SsTozU8CxYyX/6LHbgTffNGLQIBNu3BDRuLEDv/9uRv/+LiysF1KD3Y4dOo+ugHDzptIMCyi1db7cxF0cOWvGausBu2rghKpWLRlhYTKyswUcP+7a50qtreve3Y6KFbVRG18WtPUKJSKvpdMBLVqUrjk2KUnA44+b8P77AZBlASNGWLF2bRbq1NH+h3zLlg4YjTKuXBFx9qzn2qDmzzciPV1AgwYO9OrlP7Umao3dyZMiLBYPF8aFXN0UKwhlM59dVhbw3XccNFEUDHZE5DbqurElCXZbtujQqVMQtm/XIzhYxvz5ZrzzTjYCA11dSu9kMuUEY081x6ak+GdtHQBUrSojMlKCw+FdS+OVhs0GnDunfElwVbADcvezc93z9NNPeqSnC7jnHgkdO3LQxJ1o49VJRD5BDXbFGRnrcABvv23EE0+YcP26iIYNHdiwIRP9+vlPbZFKbY7dvt0zM1V9/LERGRkCGjZ0oGdP/3r+BUF7/ewuXBBgtwsICpJRubLrar3LYmTssmXKF4qhQ21+9YWiJPj0EJHbtGqlBJMzZ0Rcv3735sSkJAH9+5vw7rtK0+uwYVasW5eFunW13/RakPvuy+ln5243bghYsEA5uU6Z4l+1daqcYKeNB682w9auLbn0eKo1dseOuabZ+tgxEbt366DXyxg4kM2wd6ONVycR+YTwcKBePeVDf8+eO3/8/PmnDl26BGHrVj2CgmTMm2fGe+9lw2RyQ0G9VJs2Duh0Ms6fF3Hpknv72X38sTJ/WOPGDvTo4V+1daqcARTaqLE7dcq1/etU1avLKF9egt3umvWN1UET3brZERXln1/qioPBjojc6t57lZNIYf3sHA7gvfeMePxxE65eFdGggQMbNmTh8cf9M0zkFhKSsx7nCy8Eunw6icJcvy5g4cKc2jp/nT9MrbE7ckR0yZQ9nubqgRMqQch5nZa2OdZszhk0MXw4a+uKgsGOiNzqThMVX7smYOBAE956KwCSJGDwYKXptV4971n43tOmT89GYKCMDRv0mDAhEHY35N158wzIyhLQtKkDjzzivwG7bl1lBZCsLMGjI5NdRZ3qxFVz2OWmjowt7UTFq1frcfOmgBo1JDzwgAbStBsw2BGRW6nBbt8+XZ5Qsn27MuHw5s16mEwyPvzQjNmzsxEU5KGCeqn77nPg88/NMBplrF5twMSJgWVae3TtmoDFi5XauhdeyPbb2jpAmbJHXf9UCwMoyqrGDsjpZ7d/f+lihtoMO3iwzS9WOHEFBjsicqvYWAnlyim1HkePipAkYPZsI/r1MyEpSURMjAO//JKFgQP9t2bobjp1cmDRIjP0ehkrVxrw/PMBkMqoUvOjj4zIyhLQooUDDz3EGpNGjbQxUXFGBpCYWJbBTrnNEydEZGWV7Dbi40Xs2KGHKMoYPJjNsEXl269MIvI5opgzOvaXX/QYNMiEN95Qml7797fhl1+yUL8+m17v5uGHHZg/3wJRlPHVV0a89FKAy1ekuHpVwGefKTUmU6b4d22dqkkTbdTYnTmjnP7Ll5cQEeH6269cWUZUlARJEkr8XC1bljNookoVDpooKgY7InI7tTn2nXcC8PvvegQGypg924w5cywIDvZw4XxInz52fPihBYIg47PPjPjPf1wb7ubONcJsFtCqlQNdurC2DgAaN1aeh0OHRI8u7VZaOf3ryu5BqLV2JZmoODsb+PZbZb7GoUNZW1ccDHZE5HZqsAOAunUdWL8+C4MH21kjVAL9+9vx7rvZAJQJhN96y+iS201KEvD556ytu12DBhJEUcb16yKuXvXdJ6Us+9epSrO02Nq1eiQni6haVULnzvxSURwMdkTkdu3bO9Ctmx0jRljx669Zzg7pVDLDhtnwxhvKTLD/+18AZs8ufbjLXVvXqRNPrKqgIGV0LODbExW7I9iVZmkxddDEoEE26D2z0IrP8t1XJRH5rIAA4IsvlLVeQ0I8XRptGDvWhldfVWru3ngjAPPnG0p8W7lr6/x9JGxB1PnsfHmi4rKc6kTVtKly26dOiUhPL/r1Tp8WsHWrHoIgY8gQNsMWF4MdEZFGPP20FVOmKOFu2rRA58CH4vrwQyMsFgGtWzvw4IOsrbudugKFr9bYyXLZrTqRW6VKMqpVkyDLxRtAodbWdeniQPXqPtyR0UN881VJREQFmjzZiqefVsLdiy8G4uuvi9eOlZgoYOlS1tbdSc6asb5ZY5ecLCA1VTmwtWuXbTeI4s5nZ7UCy5crrz8OmigZBjsiIg0RBOCVV6wYN05Zb+zZZwOxcmXRw92HHxqRnS2gbVs74uJYW1cQNdidPVu8JkZvkZCghLrq1aUyX3s5Z2Rs0ULwL7/ocf26iKgoCQ89xLksS4LBjohIYwQBmDkzG8OGWSHLAp56KhBr1tw93F2+LDjnDnvhBf9dE/ZuypeXUbWq7/azc0f/OlVxR8aqtcWDB9tgKHk3Ub/GYEdEpEGCALzzTjb697fB4RDw5JOB+O23O59cP/jACKtVQLt2dnTowNq6O2nTRnl+Fi70vfThjhGxKjXYnTkj4ubNO+977pyAzZuVLyBcaaLkGOyIiDRKFIHZsy3o29cGm03AqFEmbNlScLi7dEnAl1+ytq6onn3WCkGQ8fPPBvz9t2+dSt0Z7CIigHvuUe7n4ME7f7FQX38PPmjHPfdw0ERJ+darkYiIikWvB+bNs+CRR2zIzhYwfLgJf/2V/wQ7e7ZSW9e+vR3t27O27m4aNpSc6xm7esWPsubOYAfkHkBReLCz2YCvv1aC3bBhrK0rDQY7IiKNMxiABQss6NzZjqwsAYMHm7B3b87H/4ULAr76Kqe2jopm6tRsmEwydu3SY90635hFV5Jy1ol1Rx87IHc/u8Ijx4YNeiQliahQQcLDD3PQRGkw2BER+YGAAOCzz8zo0MGOjAwBAwYEOedhmz3bCJtNQMeOdrRrx9q6oqpSRcaECUoQnjEjADYfqGi6fFmAxSLAYJBRo4Z7qhmLMjJWHbQzaJANRtesiue3GOyIiPyEyQQsXWpGmzZ2pKYKeOIJEzZs0DmbwKZMYW1dcU2caEWFChISEkTnxLreTG2GrVVLcttSXU2bKl8WLlwQcf16/s6bFy8K2LRJCX1caaL0GOyIiPxISAjw1VdmNG/uQHKyiCFDgmC3C4iLs+O++1hbV1zlygHPP68E4nfeMSIjw8MFugt3968DgNBQoG5d5bV18GD+2PHllwbIslJjHB3tQ50VvRSDnZtNmzYNHTt2RGxsrKeLQkR+KjQU+OabLDRqlBPkXngh24Ml8m3Dh9sQHS3h+nURc+d6dztizhx27g1QzZopQfL2ARR2O5z9OzlowjUY7Nysd+/eWLVqlaeLQUR+LiIC+O47Mzp1smP8eCvatHFfDY7WGAzAK68owfjjj424csV754rxRI0dUPjSYps26ZCYKCIyUkL37hw04QpeN4xn8+bNWLBgAU6dOoWMjAxERUWha9eumDhxIsqVK+fy+zt37hwWLVqEAwcOID4+HtHR0Vi9enWB+yYkJGDmzJnYt28fgoOD0bdvXzz77LMwFqOnZ+vWrV1VdCKiUqlQQcY335g9XQxN6NnTjtatHdi9W4e33jLi/fe9swbUU8FOrbG7fQDFsmXK+XPAADsCAtxaJM3yumB38+ZNNG3aFMOGDUN4eDji4+MxZ84cxMfHY/HixS6/v/j4eGzevBnNmjWDJEmQC5mMKDU1FSNGjECtWrUwZ84cJCUlYdasWbBYLJg2bZrLy0VERL5DEIDp0y3o1SsYX39twJNP2tCggXfVglqtwPnzSm2iu4Nd48YOiKKMxEQRSUkCoqJkJCYK2LBBCXpDh7IZ1lW8Ltj17ds3z/9t27aF0WjEq6++iqSkJERFReW7Tnx8PAICAlCzZs18l+3atQv169dHaGhogffXuXNndO3aFQAwdepUHD58uMD9li9fjszMTMydOxfh4eEAAIfDgddeew3jx49HVFQUHnvsMVy+fDnfdRs3boxFixbd8XETEZFva9NGQq9eNqxebcCMGQH46ivvqg09d06EJAkIDpZRqZJ7+9iFhAAxMRKOH9fhwAER3bo58NVXBkiSsoRdvXreFYJ9mU/0sVODlK2QSYLmzZuHkSNHIjExMc/2Xbt2Ydy4cVi5cmWhty2KRXsKtmzZgnbt2jnLAgDdu3eHJEnYtm0bAGDVqlXYuXNnvh+GOiIi//DKK9nQ62X89pseW7cWbeF7d0lIyKmt88SScWpz7L59OjgcOUuIcdCEa3ltsHM4HMjOzsaRI0fw0UcfoXPnzqhevXqB+86YMQMREREYOXIkbty4AQA4ePAgJkyYgB49emDEiBGlLs/p06cRHR2dZ1toaCgqVqyI06dPl/r2iYjI90VHyxgxQgkqr70WAMmLKqI81b9OpQ6gOHBAh82bdbh4UUR4uIxevThowpW8Nth16tQJTZs2Rb9+/VCxYkW89957he4bEhKChQsXwmAwYPTo0di9ezfGjRuHDh06YObMmRBc8NUkLS2twObcsLAwpKamFvl2pk6diri4OABAXFwcpkyZUuqyERGR93j+eStCQmQcPKjDqlXe0+MpZ6oTzwS73EuLLV2q1Nb1729DYKBHiqNZ3vOKu82nn34Ks9mMU6dO4eOPP8aECRPw2WefQacruGo7IiICixcvxpAhQzB06FB07NgR7733XqH7e8qsWbM8XQQiIipDFSrIeOYZK954IwBvvBGAnj3tXhFePF1j16iRBL1exvXrItatUypcOGjC9by2xq5+/fpo0aIFnnjiCcybNw87d+7Ehg0b7ngdq9WK7OxsiKIIi8UCh8N1s6iHhoYiPT093/bU1FSEhYW57H6IiMj3PfmkFVWqSLhwQcTixd6x1Jing53JBNSvr9y3LAto3drh/J9cx2uDXW6xsbEwGAw4f/58oftcvXoVo0aNQtWqVbFixQqcPXsWEydOhNXqmrUPo6Oj8/WlS09Px7Vr1/L1vSMiIv8WFARMnarMZff++wFISfFseTIygKQkzzbFAjn97ABg2DCuTVwWfCLYHThwADabrdDBEykpKRg9ejRCQkLw6aefomHDhvjss89w6NAhTJ482SU1d3Fxcdi+fTvS0tKc29avXw9RFNG+fftS3z4REWlL//52NGjgQGqqgPff9+zsu2r/ugoVJHiykUkdGRsaKqNPHw6aKAteF+wmTpyI+fPn4/fff8eOHTvw2WefYeLEiYiNjXXON3e76dOnQ5IkLFq0yDnAoV69eli0aBG2b9+OpUuXFnp/ZrMZ69evx/r163Hp0iVkZGQ4/09OTnbuN3DgQAQHB+Opp57Cn3/+iRUrVuDtt9/GwIEDC5xbj4iI/JtOB0yfrtTaLV5swLlznltqzNPNsKo+fWx44AE7Xn/dgqAgjxZFswS5sKUWPOTTTz/F2rVrcf78eciyjGrVquGhhx7CmDFjEBISUuB1Lly4AKPRWGDAOnr0KGrXrg2TyVTgdS9evIguXboUeNnSpUvRtm1b5/8JCQmYMWNGniXFJk2aVKwlxUrL4ZCQnJwJvV5EREQwUlIyYbezj4Kn8Dh4Bx4H78DjkJ8sA088YcKWLXr062fD/PkWt9zv7cfi3XeNePvtAAwZYvXa5c60SD0ODocEnc49dWleF+zozhjsvAuPg3fgcfAOPA4FO3RIRNeuQZBlAb/+monmzcv+ubn9WPzzn4FYscKAV1/NxtNPs2+bu3gi2HldUywREZGWNGki4fHHlf5kr70WAE9Up3hLUyyVPQY7IiKiMvbSS9kICJCxbZsev/3m3vlVZZnBzp8w2BEREZWx6tVljBunNIG+/noA7G4cEHrtmoD0dAGCIKNWLQY7rWOwIyIicoN//cuKiAgZJ07osHy5+yYtVqc6qVFDRoBnZ10hN2CwIyIicoOwMOC555QRqW+9ZURmpnvul82w/oXBjoiIyE1GjbLhnnskJCWJ+Phj90yVlZCgzJ/HYOcfGOyIiIjcxGgE/v1vpdZu7lwjrl4t+0mLWWPnXxjsiIiI3KhvXztatnQgK0vAO++Ufa2d2sfOk2vEkvsw2BEREbmRIOQsNfbFFwbEx5fdqdjhAM6cYY2dP2GwIyIicrN27Rx45BEbHA4BM2aUXa3dxYsCrFYBAQEyqlXjQlP+gMGOiIjIA1591QqdTsb69Qb89VfZTFp86pRymq9dW4LOvfMik4cw2BEREXlAvXoShgyxASi7pcbUEbHsX+c/GOyIiIg8ZMoUK4KCZOzZo8PPP+tdfvscEet/GOyIiIg8JCpKxlNPKUuNvfpqAE6fdu30J6dOcQ47f8NgR0RE5EH//KcVdes6kJgoom/fIBw/7rpTs1pjFx3NgRP+gsGOiIjIg0JCgB9+MKNBAweSkpRwt39/6U/PFgtw4QJr7PwNgx0REZGHVaok44cfstCypQMpKQL69Qsq9UjZhARAlgWEhsqoUIE1dv6CwY6IiMgLREQA33+fhfbt7cjIEDBggAmbNpU83J08qfyuU0eCUPYrl5GXYLAjIiLyEiEhwFdfmdG1qx1ms4Bhw0xYvbpko2XVYMepTvwLgx0REZEXMZmAJUvM6NPHBptNwNixgfjmm+KHu9w1duQ/GOyIiIi8jNEIfPKJBYMHWyFJAp5+2oTFiw3Fug0GO//EYEdEROSFdDrgf//Lxrhxyjx3U6cG4sMPi76uLIOdf2KwIyIi8lKiCMycmY3nnssGAMycGYA33jDedfmx1FTg6lXlb/ax8y8MdkRERF5MEICpU6149VUl3M2eHYB//zsA0h3ymjoxceXKEkJC3FFK8hYMdkRERD7g6aetePttCwRBxsKFRjz7bCDs9oL3TUhQJybm/HX+hsGOiIjIR4wcacPcuRbodDKWLzdg/PhAWK359zt1Sjm9s3+d/2GwIyIi8iFPPGHHwoUWGI0yfv7ZgBEjTDCb8+6jNsUy2PkfBjsiIiIf07OnHcuWmWEyydi4UY9Bg0xIT8+5XG2KrVuXTbH+hsGOiIjIB3Xq5MA335hRrpyM7dv1ePzxIKSkALLMGjt/xmBHRETko+67z4GVK7MQGSlh3z4dHn00CIcPi8jIEKDTAbVqscbO3zDYERER+bBmzST8+KMZUVESjh1Twh0A1K6trGBB/oXBjoiIyMfFxkr4+ecs1KwpIT1d6V8XE+PhQpFHMNgRERFpQK1aMn76KQv16jkAAE2aeLhA5BF6TxeAiIiIXKNqVRk//5yFtWuNGDEiwNPFIQ9gjR0REZGGREYCI0faERnp6ZKQJzDYEREREWkEgx0RERGRRjDYEREREWkEgx0RERGRRjDYEREREWkEgx0RERGRRjDYEREREWkEgx0RERGRRjDYEREREWkEgx0RERGRRjDYEREREWkEgx0RERGRRjDYEREREWmEIMuy7OlCUNHJsgxJUg6ZTifC4ZA8XCLicfAOPA7egcfBe/BYeAedToQsyxAEwS33x2BHREREpBFsiiUiIiLSCAY7IiIiIo1gsCMiIiLSCAY7IiIiIo1gsCMiIiLSCAY7IiIiIo1gsCMiIiLSCAY7IiIiIo1gsCMiIiLSCAY7IiIiIo1gsCMiIiLSCAY7IiIiIo1gsCMiIiLSCAY7H5SQkIBRo0ahefPmaN++Pd5++21YrVZPF0sTVq5cidjY2Hw/7777bp79vvvuOzz88MNo0qQJ+vTpg99//z3fbaWnp+Pll19GmzZt0KJFCzzzzDO4evWqux6KTzl37hymTZuGvn37omHDhujVq1eB+7nyed+7dy8GDBiApk2bolOnTvj0008hy7LLH5svKcpxGDZsWIHvkYSEhDz78TiUzLp16/DPf/4TcXFxaN68Ofr27Yvvv/8+33PC90LZKspx8Nb3gr74D5c8KTU1FSNGjECtWrUwZ84cJCUlYdasWbBYLJg2bZqni6cZCxcuRLly5Zz/R0VFOf9es2YNXn31VUyYMAH33Xcf1q5di4kTJ+LLL79E8+bNnfs9++yzOHXqFP7zn/8gICAAs2fPxrhx47BixQro9Xzr5RYfH4/NmzejWbNmkCSpwA8zVz7v586dw5gxY9C+fXs8++yzOHHiBN59913odDqMGTPGXQ/b6xTlOABAy5Yt8eKLL+bZVr169Tz/8ziUzJIlS1CtWjVMnToVERER2L59O1599VVcuXIFEydOBMD3gjsU5TgAXvpekMmnzJ8/X27evLmckpLi3LZ8+XK5QYMG8pUrVzxXMI1YsWKFHBMTI9+4caPQfbp16yY/99xzebYNGDBAHjt2rPP/vXv3yjExMfLWrVud2xISEuTY2Fh5zZo1ri+4j3M4HM6/X3zxRblnz5759nHl8/7qq6/KnTp1krOzs53b3nvvPfnee+/Ns83fFOU4DB06VH7yySfveDs8DiVX0GfPK6+8Irds2dJ5fPheKHtFOQ7e+l5gU6yP2bJlC9q1a4fw8HDntu7du0OSJGzbts1zBfMTFy5cwNmzZ9G9e/c823v06IEdO3Y4m8S3bNmC0NBQtG/f3rlPdHQ0GjRogC1btri1zL5AFO/8UeTq533Lli3o0qULjEZjnttKS0vDvn37XPGQfNLdjkNR8TiUXGRkZL5tDRo0QEZGBrKysvhecJO7HYei8sRxYLDzMadPn0Z0dHSebaGhoahYsSJOnz7toVJpT69evdCgQQN06dIFn3zyCRwOBwA4n+PatWvn2b9OnTqw2Wy4cOGCc7/atWtDEIQ8+0VHR/M4lYArn/esrCwkJibmex9FR0dDEAQenyLYtWsXmjdvjiZNmmDo0KHYvXt3nst5HFxrz549iIqKQkhICN8LHpT7OKi88b3Ajj4+Ji0tDaGhofm2h4WFITU11QMl0paKFSvi6aefRrNmzSAIAjZt2oTZs2cjKSkJ06ZNcz7Htx8D9X/18rS0tDx99FRhYWE4fPhwGT8K7XHl856enl7gbRmNRphMJr6P7qJ169bo27cvatWqhatXr2LRokUYNWoUli1bhhYtWgDgcXClv//+G2vXrnX24+J7wTNuPw6A974XGOyIcunYsSM6duzo/L9Dhw4ICAjA559/jgkTJniwZETe4Zlnnsnz/4MPPohevXph3rx5WLBggYdKpU1XrlzBpEmT0LZtWwwfPtzTxfFbhR0Hb30vsCnWx4SGhjrTfW6pqakICwvzQIm0r3v37nA4HDh27JjzOb79GKSlpQGA8/LQ0FBkZGTkuy0ep5Jx5fOufnu+/basVivMZjOPTzEFBQXhgQcewJEjR5zbeBxKLy0tDePGjUN4eDjmzJnj7P/I94J7FXYcCuIt7wUGOx9TUB+t9PR0XLt2LV/7PLme+hzffgxOnz4Ng8GAGjVqOPc7c+ZMvukizpw5w+NUAq583oOCglClSpV8t6Vej8en9HgcSsdisWD8+PFIT0/PN/US3wvuc6fjUFSeOA4Mdj4mLi4O27dvd347A4D169dDFMU8o27IddauXQudToeGDRuiRo0aqFWrFtavX59vn3bt2jlHNMXFxSE1NRU7duxw7nPmzBkcPXoUcXFxbi2/Frj6eY+Li8PGjRths9ny3FZoaKizbwwVTVZWFv744w80adLEuY3HoeTsdjueffZZnD59GgsXLswzhybA94K73O04FMRb3gvsY+djBg4ciGXLluGpp57C+PHjkZSUhLfffhsDBw4s0guP7mzMmDFo27YtYmNjAQAbN27Et99+i+HDh6NixYoAgKeffhqTJ09GzZo10bZtW6xduxYHDx7EF1984bydFi1aoEOHDnj55Zfx4osvIiAgAO+//z5iY2PRrVs3jzw2b2Y2m7F582YAwKVLl5CRkeE8cbVp0waRkZEufd7HjBmDn3/+Gc8//zwGDRqEkydPYtGiRZg0aVKe6Qb8zd2Og3qSe+ihh1CtWjVcvXoVn332Ga5du4YPPvjAeTs8DiX32muv4ffff8fUqVORkZGB/fv3Oy9r2LAhjEYj3wtucLfjcPDgQa99Lwjy7fWD5PUSEhIwY8YM7Nu3D8HBwejbt6/fvwldZebMmdi6dSuuXLkCSZJQq1YtPPHEExg2bFie4erfffcdFixYgMuXL6N27dp47rnn0KlTpzy3lZ6ejjfffBMbNmyA3W5Hhw4d8MorrzCAF+DixYvo0qVLgZctXboUbdu2BeDa533v3r2YNWsWjh07hsjISAwZMgTjxo3LNy2BP7nbcahcuTJef/11nDhxAjdv3oTJZEKLFi0wceJENG3aNM/+PA4l07lzZ1y6dKnAyzZu3Ohc1YDvhbJ1t+PgcDi89r3AYEdERESkEexjR0RERKQRDHZEREREGsFgR0RERKQRDHZEREREGsFgR0RERKQRDHZEREREGsFgR0RERKQRDHZEREREGsFgR0Tkg3bu3InY2Fjs3LnT00UhIi/CYEdEBGDlypWIjY3FoUOHAACbN2/GnDlzPFwq4Msvv8TKlSs9XQwi8hEMdkREBdi8eTPmzp3r6WLg66+/xqpVq/Jtb926NQ4ePIjWrVt7oFRE5K0Y7IiI3ESWZVgsFpfcliiKCAgIgCjyY5yIcvATgYjoNlOnTsWXX34JAIiNjXX+qCRJwpIlS9CzZ080adIE999/P6ZNm4bU1NQ8t9O5c2eMHz8eW7duRb9+/dC0aVMsX74cALBixQoMHz4c7dq1Q+PGjdGjRw989dVX+a4fHx+PXbt2OcswbNgwAIX3sVu3bp3zvtq2bYvJkycjKSkp3+Nr0aIFkpKS8H//939o0aIF7rvvPrz11ltwOByueRKJyCP0ni4AEZG3GTBgAK5evYpt27bh7bffznf5tGnTsGrVKvTr1w/Dhg3DxYsX8eWXX+Lo0aP4+uuvYTAYnPueOXMGzz//PAYMGID+/fujdu3aAJQm1nr16qFz587Q6/X4/fff8dprr0GWZQwZMgQA8PLLL2PGjBkICgrChAkTAAAVKlQotNwrV67ESy+9hCZNmuC5557DjRs3sHTpUuzduxc//PADQkNDnfs6HA6MGTMGTZs2xQsvvIAdO3Zg8eLFqFGjBgYPHuyS55GIPEAmIiJ5xYoVckxMjHzw4EFZlmX5tddek2NiYvLtt3v3bjkmJkb+6aef8mzfsmVLvu2dOnWSY2Ji5C1btuS7HbPZnG/b6NGj5S5duuTZ1rNnT3no0KH59v3rr7/kmJgY+a+//pJlWZatVqvcrl07uVevXrLFYnHu9/vvv8sxMTHyBx984Nz24osvyjExMfLcuXPz3Oajjz4qP/bYY/nui4h8B5tiiYiKYf369ShXrhzat2+P5ORk50+jRo0QFBSUr2m0evXq6NixY77bCQwMdP6dnp6O5ORktGnTBhcuXEB6enqxy3X48GHcuHEDgwYNQkBAgHP7gw8+iOjoaPzxxx/5rjNo0KA8/7dq1QoXL14s9n0TkfdgUywRUTGcO3cO6enpaNeuXYGX37hxI8//1atXL3C/PXv2YM6cOdi/fz/MZnOey9LT01GuXLlilevy5csA4GzqzS06Ohp79uzJsy0gIACRkZF5toWFheXrJ0hEvoXBjoioGCRJQvny5fHuu+8WePntYSl3zZzq/PnzGDlyJKKjozF16lRUqVIFBoMBmzdvxpIlSyBJUpmUPTedTlfm90FE7sdgR0RUAEEQCtxes2ZN7NixAy1btiwwtBXFpk2bYLVa8fHHH6Nq1arO7QWtIlFYOW6n3s6ZM2fy1SaeOXMmz/0QkXaxjx0RUQFMJhMAIC0tLc/27t27w+FwYN68efmuY7fb8+1fELW2TJZl57b09HSsWLGiwHIU5TYbN26M8uXLY/ny5bBarc7tmzdvRkJCAh588MG73gYR+T7W2BERFaBRo0YAgJkzZ6JDhw7Q6XTo2bMn2rRpgwEDBuCTTz7BsWPH0L59exgMBpw9exbr16/Hv//9bzzyyCN3vG31OhMmTMDAgQORmZmJ7777DuXLl8e1a9fylePrr7/GvHnzcM899yAyMrLA/n0GgwGTJ0/GSy+9hKFDh6Jnz57O6U6qVauGkSNHuuy5ISLvxWBHRFSAbt26YdiwYVizZg1++uknyLKMnj17AgBef/11NG7cGMuXL8f7778PnU6HatWqoU+fPmjZsuVdbzs6OhoffvghZs+ejbfeegsVKlTAoEGDEBkZiZdffjnPvk899RQuX76MhQsXIjMzE23atCl04Ea/fv0QGBiIBQsW4N1330VQUBC6du2KKVOm5JnDjoi0S5BztwUQERERkc9iHzsiIiIijWCwIyIiItIIBjsiIiIijWCwIyIiItIIBjsiIiIijWCwIyIiItIIBjsiIiIijWCwIyIiItIIBjsiIiIijWCwIyIiItIIBjsiIiIijWCwIyIiItKI/wcf1a9fJXx/7wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss history plot\n",
    "sns.lineplot(x=counter, y=loss_history_train, color='blue')\n",
    "sns.lineplot(x=counter, y=loss_history_val, color='red')\n",
    "plt.yscale('log')\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use the network for prediction on our task. We take as result the answer which gives the highest output (closer to 1, the correct answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, preprocess_function=preprocess_function):\n",
    "    dataset_inputids = dataset.map(preprocess_function)['input_ids']\n",
    "    dataset_inputids = [inputid for row in dataset_inputids for inputid in row]\n",
    "    dataset_train_tokenized = {'input_ids': dataset_inputids}\n",
    "    dataset_train_tokenized = Dataset.from_dict(dataset_train_tokenized).with_format('torch')\n",
    "    return dataset_train_tokenized   \n",
    "\n",
    "def predict(dataset, model):\n",
    "    # tokenize the dataset\n",
    "    dataset_tokenized = tokenize_dataset(dataset)\n",
    "\n",
    "    # create the dataloader\n",
    "    datacollator = DataCollatorWithPadding(tokenizer=fast_tokenizer)\n",
    "    generator = torch.Generator(device=mydevice)\n",
    "    dataloader = DataLoader(dataset_tokenized, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=datacollator, generator=generator)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Apply the model to the data\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(mydevice)\n",
    "            outputs = model(input_ids)\n",
    "            predictions = outputs.squeeze().cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "\n",
    "    # Group the results by lists of 8 elements\n",
    "    grouped_predictions = [all_predictions[i:i + 8] for i in range(0, len(all_predictions), 8)]\n",
    "\n",
    "    # Predict the argmax for each list\n",
    "    predicted_labels = [np.argmax(group) for group in grouped_predictions]\n",
    "\n",
    "    # Convert the argmax to the corresponding letter\n",
    "    predicted_labels = [chr(65+label) for label in predicted_labels]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def evaluate_predictions(true_labels, predictions, dataset_label=None):\n",
    "    # dataset_label is a string that specifies the dataset{'train', 'validation', 'test'}\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    print(f\"{dataset_label} Accuracy:\", accuracy)\n",
    "    print(f\"{dataset_label} F1 Score:\", f1)\n",
    "    return accuracy, f1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95888d87c7a643538e9da86aa025ae3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7323 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.1164823159907142\n",
      "train F1 Score: 0.11647329303710716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0957f029f9040eaa6eb439ece7d5a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Accuracy: 0.12083847102342787\n",
      "validation F1 Score: 0.12023731657314927\n"
     ]
    }
   ],
   "source": [
    "# now we use the original train and validation datasets\n",
    "# load the model\n",
    "BiLSTM_model = BiLSTM_Classifier(vocab_size, embedding_dim, padding_idx, out_features=out_features, lstm_layers=n_lstm_layers)\n",
    "BiLSTM_model.load_state_dict(torch.load(\"models/BiLSTM_Classifier.model\", weights_only=True, map_location=mydevice))\n",
    "\n",
    "# predict\n",
    "predicted_labels_train = predict(dataset_train, BiLSTM_model)\n",
    "train_accuracy, train_f1 = evaluate_predictions(dataset_train['answerKey'], predicted_labels_train, 'train')\n",
    "\n",
    "predicted_labels_val = predict(dataset_val, BiLSTM_model)\n",
    "val_accuracy, val_f1 = evaluate_predictions(dataset_val['answerKey'], predicted_labels_val, 'validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple LSTM is not enough, I think because this is trained from scratch and the classes don't carry a real meaning. The classes just tells if the answer is correct or not, a part from the final answer all the 8 sentences are identical. And also the model have seen just 8000 short sentences, which can be not enough for understanding language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# **[SEP]**\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **x-transformer Encoder**\n",
    "\n",
    "Now we can do exactly the same done with the LSTM, but with the Transformer Encoder only architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "TransformerWrapper                                 --\n",
      "├─TokenEmbedding: 1-1                              --\n",
      "│    └─Embedding: 2-1                              6,056,448\n",
      "├─AbsolutePositionalEmbedding: 1-2                 --\n",
      "│    └─Embedding: 2-2                              153,600\n",
      "├─Identity: 1-3                                    --\n",
      "├─Dropout: 1-4                                     --\n",
      "├─Identity: 1-5                                    --\n",
      "├─Encoder: 1-6                                     --\n",
      "│    └─ModuleList: 2-3                             --\n",
      "│    │    └─ModuleList: 3-1                        1,049,088\n",
      "│    │    └─ModuleList: 3-2                        2,100,224\n",
      "│    │    └─ModuleList: 3-3                        1,049,088\n",
      "│    │    └─ModuleList: 3-4                        2,100,224\n",
      "│    └─Identity: 2-4                               --\n",
      "│    └─LayerNorm: 2-5                              512\n",
      "│    │    └─LayerNorm: 3-5                         --\n",
      "├─Linear: 1-7                                      6,056,448\n",
      "===========================================================================\n",
      "Total params: 18,565,632\n",
      "Trainable params: 18,565,632\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n",
      "\n",
      "Ouput shape\n",
      "torch.Size([1, 300, 11829])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.pyenv/versions/3.12.3/envs/nlp-env/lib/python3.12/site-packages/x_transformers/x_transformers.py:524: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/max/.pyenv/versions/3.12.3/envs/nlp-env/lib/python3.12/site-packages/x_transformers/x_transformers.py:548: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "from x_transformers import TransformerWrapper, Encoder\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 512\n",
    "n_layers = 2\n",
    "n_heads = 8\n",
    "\n",
    "out_features = 128\n",
    "learning_rate = 1e-4\n",
    "batch_size = 128\n",
    "nb_epoch = 6\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "trf_model = TransformerWrapper(\n",
    "    num_tokens = vocab_size,\n",
    "    max_seq_len = fast_tokenizer.model_max_length,\n",
    "    attn_layers = Encoder(\n",
    "        dim = embedding_dim, # hidden dimensionality\n",
    "        depth = n_layers, # number of layers\n",
    "        heads = n_heads # number of self-attention heads\n",
    "    )\n",
    ")\n",
    "trf_model.to(mydevice)\n",
    "\n",
    "print(summary(trf_model))\n",
    "# dimension of the output of the transformer\n",
    "print('\\nOuput shape')\n",
    "print(trf_model(torch.randint(0, vocab_size, (1, fast_tokenizer.model_max_length)).to(mydevice)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerWrapper(\n",
       "  (token_emb): TokenEmbedding(\n",
       "    (emb): Embedding(11829, 512)\n",
       "  )\n",
       "  (pos_emb): AbsolutePositionalEmbedding(\n",
       "    (emb): Embedding(300, 512)\n",
       "  )\n",
       "  (post_emb_norm): Identity()\n",
       "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (project_emb): Identity()\n",
       "  (attn_layers): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm(\n",
       "            (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (1-2): 2 x None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (attend): Attend(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm(\n",
       "            (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (1-2): 2 x None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (ff): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm(\n",
       "            (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (1-2): 2 x None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (attend): Attend(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm(\n",
       "            (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "          )\n",
       "          (1-2): 2 x None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (ff): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "            )\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "    )\n",
       "    (adaptive_mlp): Identity()\n",
       "    (final_norm): LayerNorm(\n",
       "      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "    )\n",
       "  )\n",
       "  (to_logits): Linear(in_features=512, out_features=11829, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, trf_encoder, num_labels=2):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.trf_encoder = trf_encoder\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids):#, attention_mask):\n",
    "        outputs = self.trf_encoder(input_ids=input_ids)#, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # Use the output of the [CLS] token\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
