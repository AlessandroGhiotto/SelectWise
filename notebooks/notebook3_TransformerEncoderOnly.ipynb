{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div align='center'>\n",
    "<font size=\"+2\">\n",
    "\n",
    "Text Mining and Natural Language Processing  \n",
    "2023-2024\n",
    "\n",
    "<b>SelectWise</b>\n",
    "\n",
    "Alessandro Ghiotto 513944\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 - Transformer Encoder Only:\n",
    "\n",
    "1. BERT:\n",
    "    - Binary classification - NextSentencePrediction\n",
    "    - Multiclass classification - MultipleChoice\n",
    "2. Different ways of tuning a pretrained models:\n",
    "    - Linear probing\n",
    "    - Mixed method\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT**\n",
    "\n",
    "BERT is a pretrained encoder-only transformer, it encodes the input sentence, then the output of the `[CLS]` token is fed to a classification head. Each token of the input sequence have as output a vector, which can be seen as a contextualized embedding of the input token. In particular I use the `'bert-base-uncased'` model\n",
    "\n",
    "I will have 8 inputs for each sample, of this kind:\n",
    "\n",
    "**\"[CLS] fact_1 fact_2 [SEP] question [SEP] choice_i [SEP]\"** for each choice i in ['A','B','C','D','E','F','G','H']\n",
    "\n",
    "![](../imgs/3_BERT_multiplechoice_crop.png \"BERT for multiple choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3E7TUJ2EGCLQNOV1WEAJ2NN9ROPD9K',\n",
       " 'question': 'What type of water formation is formed by clouds?',\n",
       " 'choices': ['pearls',\n",
       "  'streams',\n",
       "  'shells',\n",
       "  'diamonds',\n",
       "  'rain',\n",
       "  'beads',\n",
       "  'cooled',\n",
       "  'liquid'],\n",
       " 'answerKey': 'F',\n",
       " 'fact1': 'beads of water are formed by water vapor condensing',\n",
       " 'fact2': 'Clouds are made of water vapor.',\n",
       " 'combinedfact': 'Beads of water can be formed by clouds.',\n",
       " 'formatted_question': 'What type of water formation is formed by clouds? (A) pearls (B) streams (C) shells (D) diamonds (E) rain (F) beads (G) cooled (H) liquid',\n",
       " 'answerKey_int': 5}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "# SEED\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed = 8\n",
    "set_seed(seed)\n",
    "\n",
    "# DEVICE and DTYPE\n",
    "mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(mydevice) # default tensor device\n",
    "torch.set_default_dtype(torch.float32) # default tensor dtype\n",
    "\n",
    "# DATASET\n",
    "dataset = load_dataset(\"allenai/qasc\")\n",
    "n_train_sample = 7323\n",
    "dataset_train = dataset['train'].select(range(n_train_sample))\n",
    "dataset_val = dataset['train'].select(range(n_train_sample, len(dataset['train'])))\n",
    "dataset_test = dataset['validation']\n",
    "\n",
    "def format_choices(example):\n",
    "    # transform the choices from a dictionary to a list of strings\n",
    "    # I will eliminate the labels, if we know that the order is always the same\n",
    "    # Does all the samples have the same order of choices?\n",
    "    if example['choices']['label'] == ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']:\n",
    "        # get the text of the choices\n",
    "        example['choices'] = example['choices']['text']\n",
    "    else:\n",
    "        print(\"The order of the choices is not the same for all the examples\")\n",
    "    \n",
    "    # transform the answerKey from a string to an integer\n",
    "    example['answerKey_int'] = ord(example['answerKey']) - 65\n",
    "    return example\n",
    "\n",
    "dataset_train = dataset_train.map(format_choices)\n",
    "dataset_val = dataset_val.map(format_choices)\n",
    "\n",
    "# Display the dataset\n",
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Binary Classification**\n",
    "\n",
    "I train the model like a NextSentencePrediction task (with `AutoModelForNextSentencePrediction`), I simply ask if this choice is correct or not. So as output I have just two values, one associated to the positive class and the other to the negative one.\n",
    "\n",
    "![picture](../imgs/3_BERTbinary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the `tokenizer` and the `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2755, 2487, 1998, 2755, 2475, 102, 3160, 102, 3601, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] fact1 and fact2 [SEP] question [SEP] choice [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "mydevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# EXAMPLE\n",
    "res = tokenizer('fact1 and fact2', 'question [SEP] choice')\n",
    "print(res)\n",
    "print(tokenizer.decode(res['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForNextSentencePrediction\n",
    "\n",
    "model = AutoModelForNextSentencePrediction.from_pretrained(model_name)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForNextSentencePrediction(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyNSPHead(\n",
      "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: classifier, Layer: Linear(in_features=768, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# in particular we can see the last layer -> classifier\n",
    "named_layers = list(model.named_children())\n",
    "num_last_layers = 1\n",
    "for name, layer in named_layers[-num_last_layers:]:\n",
    "    print(f\"Layer Name: {name}, Layer: {layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `preprocess_function` which takes as input batches of the dataset and tansform them, giving as output 8 inputs for each sample, in which we have `['input_ids', 'token_type_ids', 'attention_mask', 'labels']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I still don't return torch tensors and I don't pad\n",
    "# I will use the data collator, wich is memory efficient\n",
    "# In particular here that the sentences are short, I don't pad to the max length\n",
    "\n",
    "def preprocess_function_NextSentecePrediction(examples):\n",
    "    # attach fact1 and fact2\n",
    "    # and repeat each sentence 8 times to go with the 8 choices\n",
    "    first_sentences = [[f\"{examples[\"fact1\"][i]} {examples[\"fact2\"][i]}\"] * 8 for i in range(len(examples[\"fact1\"]))]\n",
    "    # Grab all second sentences, the questions.\n",
    "    questions = examples[\"question\"]\n",
    "    second_sentences = [\n",
    "        [f\"{question} [SEP] {examples[\"choices\"][i][choice_idx]}\" for choice_idx in range(8)] \n",
    "        for i, question in enumerate(questions)\n",
    "    ]\n",
    "\n",
    "    # Flatten everything\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, max_length=512,  truncation=True)\n",
    "\n",
    "    # Create the labels\n",
    "    # 1: correct choice, 0: incorrect choice\n",
    "    answerKeys = examples['answerKey'] # list of correct choices ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "    new_labels = [] # length: 8 * len(answerKeys)\n",
    "    for answerKey in answerKeys:\n",
    "        new_labels.extend([1 if chr(65+i) == answerKey else 0 for i in range(8)])\n",
    "\n",
    "    tokenized_examples['labels'] = new_labels\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the preprocessing function to the dataset\n",
    "dataset_train_encoded = dataset_train.map(preprocess_function_NextSentecePrediction, \n",
    "                                          batched=True, remove_columns=dataset_train.column_names)\n",
    "dataset_val_encoded = dataset_val.map(preprocess_function_NextSentecePrediction, \n",
    "                                      batched=True, remove_columns=dataset_val.column_names)\n",
    "# we got the input_ids, token_type_ids and attention_mask\n",
    "dataset_train_encoded.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `DataCollator`, which dinamically pad the sequences in the batch, which is more memory efficient wrt padding all the dataset to the longest sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForNextSentencePrediction:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # take the labels out\n",
    "        label_name = \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "\n",
    "        # pad\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT OBTAINED FROM THE PREPROCESS FUNCTION - sample 0\n",
      "WE CAN SEE THE PADDING AT THE END GIVEN BY THE DATACOLLATOR\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] pearls [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 0\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] streams [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 0\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] shells [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 0\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] diamonds [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 0\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] rain [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 0\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] beads [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 1\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] cooled [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 0\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] liquid [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 0\n",
      "\n",
      "CORRECT LABEL: F -> CHOICE: beads\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "# from the idx sample in the train set we got this 8 inputs\n",
    "idx = 0\n",
    "margin = 10 # create a larger batch, so we can see the padding\n",
    "features = [{k: v for k, v in dataset_train_encoded[i].items()} for i in range(idx*8, idx*8+8 + margin)]\n",
    "batch = DataCollatorForNextSentencePrediction(tokenizer)(features)\n",
    "decoded_input_ids = [tokenizer.decode(batch[\"input_ids\"][i].tolist()) for i in range(margin, margin + 8)]\n",
    "print(f'RESULT OBTAINED FROM THE PREPROCESS FUNCTION - sample {idx}\\nWE CAN SEE THE PADDING AT THE END GIVEN BY THE DATACOLLATOR\\n')\n",
    "for i in range(8):\n",
    "    print(decoded_input_ids[i])\n",
    "    print(f'label: {batch[\"labels\"][idx*8+i + margin]}')\n",
    "    print()\n",
    "print(f'CORRECT LABEL: {dataset_train['answerKey'][idx]} -> CHOICE: {dataset_train['choices'][idx][ord(dataset_train[\"answerKey\"][idx])-65]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "batch_size = 16\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 1\n",
    "# ---------------------------------------------\n",
    "\n",
    "# DATALOADERS\n",
    "generator = torch.Generator(device=mydevice)\n",
    "train_dataloader = DataLoader(dataset_train_encoded, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=DataCollatorForNextSentencePrediction(tokenizer), generator=generator)\n",
    "val_dataloader = DataLoader(dataset_val_encoded, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=DataCollatorForNextSentencePrediction(tokenizer), generator=generator)\n",
    "\n",
    "# OPTIMIZER\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# SCHEDULER\n",
    "num_training_steps = num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a656c7e4e7414ab4b260c2fa9ab07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm. import tqdm\n",
    "\n",
    "mydevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(mydevice)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # move the batch to cuda\n",
    "        batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "# save the model\n",
    "# model.save_pretrained(f\"../models/{model_name}-NextSentencePrediction-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9824290998766955}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForNextSentencePrediction.from_pretrained(f\"../models/{model_name}-NextSentencePrediction-finetuned\")\n",
    "model.to(mydevice)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in val_dataloader:\n",
    "    batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1405,  1.0166],\n",
      "        [ 3.3760, -2.8730],\n",
      "        [ 3.3582, -2.7215],\n",
      "        [ 4.1679, -3.7248],\n",
      "        [ 3.6158, -3.1926],\n",
      "        [ 3.9187, -3.4710],\n",
      "        [ 3.3933, -2.9008],\n",
      "        [ 3.5048, -2.9808],\n",
      "        [-0.8759,  2.9226],\n",
      "        [ 4.1225, -3.7218],\n",
      "        [ 3.1081, -2.4220],\n",
      "        [ 4.1965, -3.7751],\n",
      "        [ 3.1352, -2.5844],\n",
      "        [ 3.1694, -2.6081],\n",
      "        [ 4.3199, -3.9138],\n",
      "        [-0.3883,  1.6641]], device='cuda:0')\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "batch = next(iter(val_dataloader))\n",
    "batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "print(logits)\n",
    "print(predictions)\n",
    "print(batch[\"labels\"])\n",
    "print(predictions == batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very good, and we can also see from the logits values that the model is also quite confident in what it's predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen the prediction for each input, given by splitting in eight each sample. Our task is not just to see if this is correct or not, but at the end I pick the most correct, and see if this is correct.\n",
    "\n",
    "For each sample I will choose the answer which as the highest `logits[1]` (the sentence which is more probable to be correct). I look at the second dimension of the logits because is the one associated to the positive label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answerKey_predictions(dataset_encoded, model):\n",
    "    # I use dataloaders with shuffle=False and batch_size=8\n",
    "    # so each batch is exactly one of the original samples\n",
    "    batch_size = 8\n",
    "    generator = torch.Generator(device=mydevice)\n",
    "    dataloader = DataLoader(dataset_encoded, batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=DataCollatorForNextSentencePrediction(tokenizer), generator=generator)\n",
    "    \n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        logits_for_positive_class = logits[:, 1]\n",
    "        prediction = torch.argmax(logits_for_positive_class)#, dim=-1)\n",
    "        predictions.append(prediction.cpu().numpy())\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def evaluate_predictions(true_labels, predictions, dataset_label=None):\n",
    "    # dataset_label is a string that specifies the dataset{'train', 'validation', 'test'}\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    print(f\"{dataset_label} Accuracy:\", accuracy)\n",
    "    print(f\"{dataset_label} F1 Score:\", f1)\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.9786972552232691\n",
      "train F1 Score: 0.9786639385715545\n",
      "validation Accuracy: 0.9741060419235512\n",
      "validation F1 Score: 0.9736600249668403\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "predictions_train = answerKey_predictions(dataset_train_encoded, model)\n",
    "train_accuracy, train_f1 = evaluate_predictions(dataset_train['answerKey_int'], predictions_train, 'train')\n",
    "\n",
    "# validation\n",
    "predictions_val = answerKey_predictions(dataset_val_encoded, model)\n",
    "val_accuracy, val_f1 = evaluate_predictions(dataset_val['answerKey_int'], predictions_val, 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Multiclass Classification**\n",
    "\n",
    "I train the model like a single-label multi-class classification task (with `AutoModelForMultipleChoice`), for each choice I have an output, the one which is higher is the correct choice (given by the model). Now I have as output 8 values, I take the argmax, and gives me a label in {0, 1, 2, 3, 4, 5, 6, 7}\n",
    "\n",
    "![picture](../imgs/3_BERTmulti.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "\n",
    "mydevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "set_seed(seed)\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMultipleChoice(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: classifier, Layer: Linear(in_features=768, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# in particular we can see the last layer -> classifier\n",
    "named_layers = list(model.named_children())\n",
    "num_last_layers = 1\n",
    "for name, layer in named_layers[-num_last_layers:]:\n",
    "    print(f\"Layer Name: {name}, Layer: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def preprocess_function_MultipleChoice(examples):\n",
    "    # attach fact1 and fact2\n",
    "    # and repeat each sentence 8 times to go with the 8 choices\n",
    "    first_sentences = [[f\"{examples[\"fact1\"][i]} {examples[\"fact2\"][i]}\"] * 8 for i in range(len(examples[\"fact1\"]))]\n",
    "    # Grab all second sentences, the questions.\n",
    "    questions = examples[\"question\"]\n",
    "    second_sentences = [\n",
    "        [f\"{question} [SEP] {examples[\"choices\"][i][choice_idx]}\" for choice_idx in range(8)] \n",
    "        for i, question in enumerate(questions)\n",
    "    ]\n",
    "\n",
    "    # Flatten everything\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    # Un-flatten -> each example has 8 choices\n",
    "    tokenized_examples = {k: [v[i:i+8] for i in range(0, len(v), 8)] for k, v in tokenized_examples.items()}\n",
    "\n",
    "    # Create the labels\n",
    "    # ['A','B','C','D','E','F','G','H'] -> [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    answerKeys = examples['answerKey'] \n",
    "    tokenized_examples['labels'] = [ord(answerKey) - ord('A') for answerKey in answerKeys]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "dataset_train_encoded = dataset_train.map(preprocess_function_MultipleChoice, \n",
    "                                          batched=True, remove_columns=dataset_train.column_names)\n",
    "dataset_val_encoded = dataset_val.map(preprocess_function_MultipleChoice, \n",
    "                                      batched=True, remove_columns=dataset_val.column_names)\n",
    "# we got the input_ids, token_type_ids and attention_mask\n",
    "dataset_train_encoded.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # take the labels out\n",
    "        label_name = \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "\n",
    "        # flatten (because now I have a list of 8 choices for each example)\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        # pad\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT OBTAINED FROM THE PREPROCESS FUNCTION - sample 0\n",
      "WE CAN SEE THE PADDING AT THE END GIVEN BY THE DATACOLLATOR\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] pearls [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 5\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] streams [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 5\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] shells [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 5\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] diamonds [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 5\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] rain [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 5\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] beads [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 5\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] cooled [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 5\n",
      "\n",
      "[CLS] beads of water are formed by water vapor condensing clouds are made of water vapor. [SEP] what type of water formation is formed by clouds? [SEP] liquid [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: 5\n",
      "\n",
      "CORRECT LABEL: F -> CHOICE: beads\n"
     ]
    }
   ],
   "source": [
    "# The difference now is that all the choices are stored in the same sample\n",
    "# EXAMPLE\n",
    "idx = 0\n",
    "margin = 10 # create a larger batch, so we can see the padding\n",
    "features = [{k: v for k, v in dataset_train_encoded[i].items()} for i in range(idx, idx + margin)]\n",
    "batch = DataCollatorForMultipleChoice(tokenizer)(features)\n",
    "decoded_input_ids = [tokenizer.decode(batch[\"input_ids\"][idx][i].tolist()) for i in range(8)]\n",
    "print(f'RESULT OBTAINED FROM THE PREPROCESS FUNCTION - sample {idx}\\nWE CAN SEE THE PADDING AT THE END GIVEN BY THE DATACOLLATOR\\n')\n",
    "for i in range(8):\n",
    "    print(decoded_input_ids[i])\n",
    "    print(f'label: {batch[\"labels\"][idx]}')\n",
    "    print()\n",
    "print(f'CORRECT LABEL: {dataset_train['answerKey'][idx]} -> CHOICE: {dataset_train['choices'][idx][ord(dataset_train[\"answerKey\"][idx])-65]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "batch_size = 16\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 1\n",
    "# ---------------------------------------------\n",
    "\n",
    "# DATALOADERS\n",
    "generator = torch.Generator(device=mydevice)\n",
    "train_dataloader = DataLoader(dataset_train_encoded, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=DataCollatorForMultipleChoice(tokenizer), generator=generator)\n",
    "val_dataloader = DataLoader(dataset_val_encoded, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=DataCollatorForMultipleChoice(tokenizer), generator=generator)\n",
    "\n",
    "# OPTIMIZER\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# SCHEDULER\n",
    "num_training_steps = num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa94c2640b7d4d64a679cdd7f6ee0b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "mydevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(mydevice)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # move the batch to cuda\n",
    "        batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "# save the model\n",
    "# model.save_pretrained(f\"../models/{model_name}-MultipleChoice-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.9793800355045746\n",
      "train F1 Score: 0.9793611480842821\n",
      "validation Accuracy: 0.9790382244143033\n",
      "validation F1 Score: 0.9791321003700275\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def evaluate_model(dataloader, model, dataset_label=None):\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        # Add the batch predictions and references to the metrics\n",
    "        accuracy_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        f1_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    # Compute and print the results\n",
    "    accuracy_result = accuracy_metric.compute()\n",
    "    f1_result = f1_metric.compute(average='macro')\n",
    "    print(f\"{dataset_label} Accuracy: {accuracy_result['accuracy']}\")\n",
    "    print(f\"{dataset_label} F1 Score: {f1_result['f1']}\")\n",
    "    return accuracy_result['accuracy'], f1_result['f1']\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(f\"../models/{model_name}-MultipleChoice-finetuned\")\n",
    "model.to(mydevice)\n",
    "\n",
    "# train\n",
    "train_accuracy, train_f1 = evaluate_model(train_dataloader, model, 'train')\n",
    "\n",
    "# validation\n",
    "val_accuracy, val_f1 = evaluate_model(val_dataloader, model, 'validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this classification head is just sufficient to take the argmax for each sample, the model gives directly the asnwer that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.0935,  3.9343, -3.8873, -4.2520, -2.5385,  5.5411, -3.2184, -3.9984],\n",
      "        [-4.4275,  5.1825, -3.8262, -4.0298, -4.8384, -3.1587, -2.5576, -3.8295],\n",
      "        [-4.7901, -3.9886, -2.1845,  7.3923, -4.1852, -4.0817, -2.0705, -3.7491],\n",
      "        [-3.4828, -4.6891,  4.8587, -4.9498, -3.7593, -2.3122, -3.7485, -2.1878],\n",
      "        [-2.8561, -4.0060,  7.3995, -2.7483, -4.4307, -3.8597, -4.5980, -2.7486],\n",
      "        [-3.4716, -3.6285, -3.3278, -1.8094, -4.0485, -1.5041,  2.7307,  5.0338],\n",
      "        [-3.6403,  7.3583, -3.5429, -1.0634, -4.0818, -2.4636, -3.0010, -1.3780],\n",
      "        [-2.0641,  7.4481, -3.8136, -2.2357, -4.2170, -3.6363, -2.1736, -4.5679],\n",
      "        [ 4.0983, -3.1642, -4.0581, -0.6500, -3.7150, -4.4371, -3.6834,  6.6779],\n",
      "        [-4.4480, -4.3171, -1.9642, -4.2828,  5.6323, -4.5545, -4.6879, -4.2238],\n",
      "        [ 5.4727, -3.9283, -4.5002, -3.8473, -2.7156, -2.2504, -4.0748, -4.0479],\n",
      "        [-3.3253,  4.6846, -3.9729, -3.9657, -3.6748, -4.0582, -4.5269, -4.4246],\n",
      "        [-2.5622,  7.4382, -1.3606, -4.0049, -1.1434, -1.9810, -4.2903, -3.6334],\n",
      "        [-0.9376,  7.4581, -2.8129, -1.6168, -4.6943, -2.4021, -1.7703, -2.8352],\n",
      "        [-3.7758, -2.4886,  4.6790, -4.4459, -4.0989, -4.6330, -1.8951, -1.7153],\n",
      "        [-4.3448,  0.3117, -5.0902, -1.5539,  6.2243, -0.6918, -1.2927, -3.3182]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 1, 3, 2, 2, 7, 1, 1, 7, 4, 0, 1, 1, 1, 2, 4], device='cuda:0')\n",
      "tensor([5, 1, 3, 2, 2, 7, 1, 1, 7, 4, 0, 1, 1, 1, 2, 4], device='cuda:0')\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "batch = next(iter(val_dataloader))\n",
    "batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "print(logits)\n",
    "print(predictions)\n",
    "print(batch[\"labels\"])\n",
    "print(predictions == batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "Binary Classification: validation accuracy = $0.97410$\n",
    "\n",
    "Multiclass Classification: validation Accuracy = $0.97903$\n",
    "\n",
    "The results are almost identical, for the next steps I will use the second (`AutoModelForMultipleChoice`), which is much easier and confortable to handle. With all the choices in one sample is much more cleane (also for the predictions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Different ways of tuning a pretrained models**\n",
    "\n",
    "![picture](../imgs/3_LinearProbing_FineTuning.png \"LinearProbing & FineTuning \")\n",
    "\n",
    "Until now I have **fine-tuned** the models, I have updated all the weights while training.\n",
    "\n",
    "## **Linear probing**\n",
    "\n",
    "With linear probing we train just the classification head weights, and the representation of the point given by the model doesn't change, are fixed in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine-tuning is better than linear probing (we train all the weights)\n",
    "\n",
    "linear probing should performe better out of distribution, because the pretrained features are fixed.\n",
    "\n",
    "the `DataCollactor` and the `encoded dataset` are taken from the previous part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "\n",
    "mydevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "set_seed(seed)\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # WHAT WE DON'T TRAIN -> FREEZE\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = False\n",
    "    # THE OTHERS ARE THE ONE TO BE TRAINED\n",
    "    else:  # classifier layer\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "batch_size = 16\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 2\n",
    "# ---------------------------------------------\n",
    "\n",
    "# DATALOADERS\n",
    "generator = torch.Generator(device=mydevice)\n",
    "train_dataloader = DataLoader(dataset_train_encoded, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=DataCollatorForMultipleChoice(tokenizer), generator=generator)\n",
    "val_dataloader = DataLoader(dataset_val_encoded, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=DataCollatorForMultipleChoice(tokenizer), generator=generator)\n",
    "\n",
    "# OPTIMIZER\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# SCHEDULER\n",
    "num_training_steps = num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76102c57a96241c791a0898503dfc6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "mydevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(mydevice)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "# save the model\n",
    "# model.save_pretrained(f\"../models/{model_name}-MultipleChoice-linearprobing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def evaluate_model(dataloader, model, dataset_label=None):\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        # Add the batch predictions and references to the metrics\n",
    "        accuracy_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        f1_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    # Compute and print the results\n",
    "    accuracy_result = accuracy_metric.compute()\n",
    "    f1_result = f1_metric.compute(average='macro')\n",
    "    print(f\"{dataset_label} Accuracy: {accuracy_result['accuracy']}\")\n",
    "    print(f\"{dataset_label} F1 Score: {f1_result['f1']}\")\n",
    "    return accuracy_result['accuracy'], f1_result['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.9980882152123447\n",
      "train F1 Score: 0.9980920146845049\n",
      "validation Accuracy: 0.9790382244143033\n",
      "validation F1 Score: 0.978762918439323\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(f\"../models/{model_name}-MultipleChoice-linearprobing\")\n",
    "model.to(mydevice)\n",
    "\n",
    "# train\n",
    "train_accuracy, train_f1 = evaluate_model(train_dataloader, model, 'train')\n",
    "\n",
    "# validation\n",
    "val_accuracy, val_f1 = evaluate_model(val_dataloader, model, 'validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.0635, -8.1475, -8.2077, -7.7835,  7.1177, -7.9665, -8.1542, -3.8058],\n",
      "        [-7.8387, -1.3881,  7.9918,  7.8251, -8.2508, -7.2913, -8.2615, -8.1033],\n",
      "        [-7.1352, -7.9718, -8.0846, -8.1383,  5.0578, -8.2606, -8.3322, -7.4598],\n",
      "        [ 6.5435,  1.7675, -6.9395, -7.8207, -8.0814, -7.0317, -8.0152, -7.5562],\n",
      "        [-7.3153, -5.1316, -8.0707, -7.9625, -8.0620,  7.6504, -8.2321, -7.9300],\n",
      "        [ 5.2820, -8.2117, -8.1995, -8.0062, -8.1868, -7.9580, -7.9542, -7.7910],\n",
      "        [-8.2587, -8.2831, -8.1234,  8.3343, -8.2293, -8.1069, -8.0794, -8.2049],\n",
      "        [-8.1586, -8.2209, -8.1853,  8.5138, -8.2739, -8.2566, -6.7375, -8.0082],\n",
      "        [ 0.0820, -0.1621, -0.6542,  6.5795, -5.5367, -6.3054, -4.4811,  3.0971],\n",
      "        [-8.2234, -8.1691, -8.1770, -8.2145, -7.7564,  7.8036, -8.2401, -8.1932],\n",
      "        [-8.1832, -8.2644, -8.1885, -8.2319, -8.1982, -7.3070,  7.8420, -8.1723],\n",
      "        [-7.8365, -8.2693, -8.2553, -8.2627,  7.0586, -8.1292, -8.2239, -8.1727],\n",
      "        [ 8.4885, -7.7304, -8.2043, -8.2429, -8.1859, -7.7777, -6.7501, -7.7813],\n",
      "        [-8.2510, -8.0913, -8.1626,  8.1393, -7.9259, -8.1275, -8.0233, -8.1660],\n",
      "        [-8.2898,  8.2922, -7.6048, -8.0096, -5.8473, -7.4310, -7.7279, -8.2458],\n",
      "        [-7.4513, -8.2020, -5.5014, -8.1563, -8.2024,  5.4736, -8.2109, -8.2408]],\n",
      "       device='cuda:0')\n",
      "tensor([4, 2, 4, 0, 5, 0, 3, 3, 3, 5, 6, 4, 0, 3, 1, 5], device='cuda:0')\n",
      "tensor([4, 3, 4, 0, 5, 0, 3, 3, 3, 5, 6, 4, 0, 3, 1, 5], device='cuda:0')\n",
      "tensor([ True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "batch = next(iter(val_dataloader))\n",
    "batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "print(logits)\n",
    "print(predictions)\n",
    "print(batch[\"labels\"])\n",
    "print(predictions == batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Combined method**\n",
    "\n",
    "![picture](../imgs/3_BERT_combinedmethod.png)\n",
    "\n",
    "Take the best from the two methods\n",
    "\n",
    "1) We train the classification head only, and we learn the linear classification boundary (I simply take the model trained with linear probing)\n",
    "\n",
    "2) We fix the classification head, and train the rest\n",
    "\n",
    "So in this way we have nothing to lose, first we create the boundary and then try to adapt the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "\n",
    "mydevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "set_seed(seed)\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(f\"../models/{model_name}-MultipleChoice-linearprobing\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # SET THE WEIGHTS TRAINABLE IN ALL THE OTHER LAYERS\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = True\n",
    "    # FREEZE THE CLASSIFIER\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fc8f6c26844743a11207fb83405ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/458 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "batch_size = 16\n",
    "learning_rate = 5e-6 # just a little adjustment\n",
    "num_train_epochs = 1\n",
    "# ---------------------------------------------\n",
    "\n",
    "# OPTIMIZER\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# SCHEDULER\n",
    "num_training_steps = num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(mydevice) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "# save the model\n",
    "# model.save_pretrained(f\"../models/{model_name}-MultipleChoice-combinedmethod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0.9989075515499113\n",
      "train F1 Score: 0.9989121147081581\n",
      "validation Accuracy: 0.9815043156596794\n",
      "validation F1 Score: 0.9813588250400052\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(f\"../models/{model_name}-MultipleChoice-combinedmethod\")\n",
    "model.to(mydevice)\n",
    "\n",
    "# train\n",
    "train_accuracy, train_f1 = evaluate_model(train_dataloader, model, 'train')\n",
    "\n",
    "# validation\n",
    "val_accuracy, val_f1 = evaluate_model(val_dataloader, model, 'validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**\n",
    "\n",
    "| Metric          |Fine-tuning|Linear probing|Combined method|\n",
    "|-----------------|-----------|--------------|---------------|\n",
    "| Val Accuracy    | $0.97903$ | $0.97903$    | $0.98150$     | \n",
    "| Val F1 Score    | $0.97913$ | $0.97876$    | $0.98135$     |\n",
    "\n",
    "\n",
    "Ranking (based on the accuracy and f1 on the validation set):\n",
    "\n",
    "1) combined method\n",
    "2) fine-tuning\n",
    "3) linea probing\n",
    "\n",
    "But at the end the results are almost identical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
